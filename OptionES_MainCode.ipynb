{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log as ln\n",
    "from numpy import nan\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import glob\n",
    "from numpy import sqrt\n",
    "from msilib.schema import Error\n",
    "pd.options.mode.chained_assignment = None\n",
    "source = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class oes():\n",
    "\n",
    "    def import_crsp(path,low_memory_=True):\n",
    "        \"\"\"Input path for a CRSP or Ivolatility file. This function will read the csv and drop 'Unnamed: 0' column. Rename 'date' to 'DataDate' and convert the data to datetime format.\"\"\"\n",
    "        df = pd.read_csv(path,low_memory=low_memory_)\n",
    "        \n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop(columns= 'Unnamed: 0')\n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            df = df.rename(columns= {\"date\":\"DataDate\"})\n",
    "\n",
    "        df.DataDate = pd.to_datetime(df.DataDate)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def import_ivol(ticker,folder):\n",
    "        old = pd.DataFrame()\n",
    "        paths = glob.glob(f\"{folder}{ticker}_*.csv\")\n",
    "        if len(paths) != 0:\n",
    "            for path in paths:\n",
    "                df = oes.import_crsp(path)\n",
    "                old = pd.concat([old,df])\n",
    "            old.sort_values(\"DataDate\")[['UnderlyingSymbol', 'UnderlyingPrice', 'Type', 'Expiration', 'DataDate','Strike', 'Bid', 'Ask', 'Volume', 'OpenInterest']]\n",
    "            return(old)\n",
    "        else:\n",
    "            return(pd.DataFrame())\n",
    "\n",
    "    def dupe_check(df_crsp,path_crsp):\n",
    "        \"\"\"This check looks for duplicate dates in the CRSP file. If in case the number of duplicate dates is more than 8 days in an year (ie 3%), then the check will open the corresponding csv file for CRSP and IVOL database.\n",
    "        Note: Do not forget to check for different ivol files in case of multiple shareclass. \"\"\"\n",
    "        dupes = df_crsp[df_crsp.DataDate.duplicated(keep=False)]\n",
    "        if len(dupes) == 0:\n",
    "            return df_crsp\n",
    "        \n",
    "        # if dupes.DIVAMT.isnull().values.any():\n",
    "        if len(dupes) > len(df_crsp)*0.03:\n",
    "            print(\"dupe\")\n",
    "            os.startfile(path_crsp)\n",
    "            input(\"Press enter to continue...\")\n",
    "            corrected_df = oes.import_crsp(path_crsp)\n",
    "        \n",
    "        else:\n",
    "            corrected_df = df_crsp.groupby('DataDate').last()\n",
    "            divamt = df_crsp.groupby('DataDate').sum().DIVAMT\n",
    "            corrected_df.DIVAMT = divamt\n",
    "            corrected_df = corrected_df.reset_index()\n",
    "        \n",
    "        return(corrected_df)\n",
    "\n",
    "    def return_check(df,path):\n",
    "        df = df.sort_values(by=\"DataDate\")\n",
    "        df = df.reset_index()\n",
    "        if df['RET'].iloc[0] == \"C\":\n",
    "            df.loc[0,'RET'] = nan\n",
    "        if \"C\" in df.RET.tolist():\n",
    "            print(\"Ret\")\n",
    "            os.startfile(path)\n",
    "            input(\"Press enter to continue...\")\n",
    "            df = oes.import_crsp(path)\n",
    "        return(df)\n",
    "\n",
    "    def regression(df, dtype):\n",
    "        data_type = dtype + \"LogRet\"                                                #Adding a suffix to signify Log Returns \n",
    "        formula = data_type + \" ~  Mkt_Lnrtn\"                                       #Creating the formula for Regression\n",
    "        years = df.index.year.unique().tolist()                                     #Creating a list of years for yearly regression\n",
    "\n",
    "        if 2012 in years:\n",
    "            years.remove(2012)\n",
    "\n",
    "        df_reg = pd.DataFrame()                                                     #Empty df to concat yearly data to \n",
    "\n",
    "        for y in years:                                                             #Running a for loop for yearly regressions\n",
    "            df_year = df.loc[f\"{y}\"]                                                #Filtering for a specific year\n",
    "            try:\n",
    "                if len(df_year.dropna(subset=\"SynthLogRet\")) < 250:                 #Removing incomplete years \n",
    "                    continue\n",
    "            except:\n",
    "                if len(df_year.dropna(subset=\"CompanyLogRet\")) < 250:               #Removing incomplete years \n",
    "                    continue\n",
    "            fitted = ols(formula, data = df_year).fit()                             #Running the regression on the filtered year  \n",
    "            explained_rtn = fitted.predict(exog = df_year)                          #Calculating the expected returns \n",
    "            df_year.loc[:,\"ExplainedReturn\"] = explained_rtn                        #Assigned a column to the expected returns \n",
    "            df_year.loc[:,\"ResRtn\"] = df_year[data_type] - explained_rtn            #Calculating Residual Returns\n",
    "            df_year.loc[:,\"Tstat\"] = df_year.ResRtn/sqrt(fitted.scale)              #Calculating Tstat\n",
    "            df_year.loc[:,\"Sig\"] = abs(df_year.Tstat) > 1.96                        #Checking significance\n",
    "            df_year.loc[:,f\"StdErr_{dtype}\"] = fitted.bse[0]\n",
    "            df_reg = pd.concat([df_reg,df_year])                                    #Joining the yearly regressions  \n",
    "        if len(df_reg) == 0:\n",
    "            return(\"Lack of data\")\n",
    "        return(df_reg)\n",
    "\n",
    "    def blr(blr_path):\n",
    "        df = pd.read_excel(blr_path, skiprows = 5).drop(columns = [\"Date\",\"PX_LAST.1\",\"PX_LAST\",\"Unnamed: 2\"]).rename(columns={\"Date.1\":\"DataDate\",\"rate\":\"blr\"})\n",
    "        return(df)\n",
    "\n",
    "    def fred(fred_path):\n",
    "        df = pd.read_excel(fred_path,skiprows=10).rename(columns={\"observation_date\":\"DataDate\",\"DGS1\":\"fred\"})\n",
    "        df.fred = df.fred/100\n",
    "        df = df.fillna(method=\"ffill\")\n",
    "        return(df)\n",
    "\n",
    "    def marketdata(path):\n",
    "        df = oes.import_crsp(path)\n",
    "        df.loc[:,\"Mkt_Lnrtn\"] = ln(1 + df.vwretd)\n",
    "        market = df[[\"DataDate\",\"Mkt_Lnrtn\"]].groupby(\"DataDate\").mean()\n",
    "        market.sort_index(inplace= True)\n",
    "        return(market)\n",
    "\n",
    "    def synthetic_stock(syn_df,cfacpr,fred,blr,volume_filter,oi_filter,zero_price_filter):\n",
    "\n",
    "        if volume_filter == True:   \n",
    "            syn_df = syn_df[syn_df.Volume != 0]                                     #Filtering for volume if volume_filter is True\n",
    "        \n",
    "        if oi_filter == True:\n",
    "            syn_df = syn_df[syn_df.OpenInterest != 0]                               #Filtering for open interest if oi_filter is True\n",
    "\n",
    "        syn_df.DataDate = pd.to_datetime(syn_df.DataDate)                           #Converting dates from str to datetime\n",
    "        syn_df.Expiration = pd.to_datetime(syn_df.Expiration)                       #Converting dates from str to datetime\n",
    "\n",
    "        syn_df = syn_df.merge(fred, on = 'DataDate', how = \"left\")                  #Joining fred data with df\n",
    "        syn_df = syn_df.merge(blr, on = 'DataDate', how = \"left\")                   #Joining blr data with df\n",
    "        \n",
    "        syn_df.loc[:,\"Tau\"] = (syn_df.Expiration - syn_df.DataDate).dt.days         #Calculating Tau\n",
    "        syn_df.loc[:,\"YearFrac\"] =  syn_df.Tau/365                                  #Converting Tau into years\n",
    "\n",
    "        syn_df.loc[:,\"BuyDiscount\"] = ((1 + syn_df.fred) ** syn_df.YearFrac)        #Calculating Buy Discount\n",
    "        syn_df.loc[:,\"SellDiscount\"] = (1 + syn_df.blr) ** syn_df.YearFrac          #Calculating Sell Discount\n",
    "        \n",
    "        syn_df.loc[:,\"Spread\"] = abs(syn_df.Ask - syn_df.Bid)\n",
    "        syn_df.loc[:,\"SpreadPercentage\"] = syn_df.Spread/((syn_df.Bid+syn_df.Ask)/2)\n",
    "        syn_df = syn_df[syn_df.SpreadPercentage < 1]\n",
    "\n",
    "        call = syn_df[syn_df.Type == \"call\"].drop(columns = \"Type\")                 #Splitting the df into calls and puts\n",
    "        call = call.rename(columns={'Last':'CallLast', 'Bid': \"CallBid\",\\\n",
    "            'Ask':\"CallAsk\", 'Volume':\"CallVolume\", 'OpenInterest':\\\n",
    "                'CallOpenInterest', 'IV':\"CallIV\", 'Delta':'CallDelta',\\\n",
    "                    'Gamma':'CallGamma', 'Theta':'CallTheta', 'Vega':'CallVega'})   #Renaming the columns\n",
    "        \n",
    "        put = syn_df[syn_df.Type == \"put\"].drop(columns = \"Type\")                   #Splitting the df into calls and puts\n",
    "        put = put.rename(columns={'Last':'PutLast', 'Bid': \"PutBid\", \\\n",
    "            'Ask':\"PutAsk\", 'Volume':\"PutVolume\", 'OpenInterest':\\\n",
    "                'PutOpenInterest', 'IV':\"PutIV\", 'Delta':'PutDelta',\\\n",
    "                    'Gamma':'PutGamma', 'Theta':'PutTheta', 'Vega':'PutVega'})      #Renaming the columns\n",
    "        \n",
    "        syn_df = call.merge(put,how = \"left\", on=[\"Expiration\",\"DataDate\",\\\n",
    "            \"Strike\", \"UnderlyingSymbol\",\"UnderlyingPrice\",\"BuyDiscount\",\\\n",
    "                \"SellDiscount\",\"YearFrac\",\"Tau\",\"blr\",\"fred\"])                      #Merging the call and put df to make them parallel\n",
    "\n",
    "        if zero_price_filter == True:                                               #Filtering for zero price quotes if zero_price_filter is True \n",
    "            syn_df = syn_df[syn_df[\"CallBid\"] != 0]\n",
    "            syn_df = syn_df[syn_df[\"PutBid\"] != 0]\n",
    "            syn_df = syn_df[syn_df[\"CallAsk\"] != 0]\n",
    "            syn_df = syn_df[syn_df[\"PutAsk\"] != 0]\n",
    "\n",
    "        syn_df = syn_df[syn_df[\"CallBid\"] != 9999]\n",
    "        syn_df = syn_df[syn_df[\"PutBid\"] != 9999]\n",
    "        syn_df = syn_df[syn_df[\"CallAsk\"] != 9999]\n",
    "        syn_df = syn_df[syn_df[\"PutAsk\"] != 9999]\n",
    "\n",
    "        abs(syn_df.CallAsk - syn_df.PutBid)/syn_df.CallAsk \n",
    "\n",
    "        syn_df.loc[:,\"Buy\"] = syn_df.Strike/syn_df.BuyDiscount                      #Calculating Buy price for the bond\n",
    "        \n",
    "        syn_df.loc[:,\"Sell\"] = syn_df.Strike/syn_df.SellDiscount                    #Calculating Sell price for the bond\n",
    "        \n",
    "        syn_df.loc[:,\"SynthAsk\"] = syn_df.CallAsk - \\\n",
    "            syn_df.PutBid + syn_df.Buy                                              #Calculating Synthetic stock's ask price\n",
    "        \n",
    "        syn_df.loc[:,\"SynthBid\"] = syn_df.CallBid - \\\n",
    "            syn_df.PutAsk + syn_df.Sell                                             #Calculating Synthetic stock's sell price\n",
    "        \n",
    "        syn_df.loc[:,\"SynthPrice\"] = (syn_df.SynthAsk + syn_df.SynthBid)/2          #Calculating Synthetic stock's price for the specific strike price on a day\n",
    "\n",
    "        syn_df = syn_df.groupby(\"DataDate\").mean()                                  #Calculating Synthetic stock's price on a day\n",
    "        \n",
    "        syn_df = syn_df.merge(cfacpr,on='DataDate',how='left')\n",
    "        \n",
    "        return(syn_df)\n",
    "\n",
    "    def summarize(mkt_crsp,mkt_ivol,tic,filt):\n",
    "        global com_after_cleaning,syn_after_cleaning,com_after_regression,syn_after_regression,summary_count\n",
    "        mkt_crsp.loc[:,\"CompanyLogRet\"] = ln(mkt_crsp.RET.astype(float) + 1)\n",
    "        mkt_ivol.loc[:,\"SynthLogRet\"] = ln(((mkt_ivol.SynthPrice*(mkt_ivol.CFACPR.shift(1)/mkt_ivol.CFACPR))/mkt_ivol.SynthPrice.shift(1)))\n",
    "        com_after_cleaning += len(mkt_crsp.dropna(subset = \"CompanyLogRet\"))\n",
    "        syn_after_cleaning += len(mkt_ivol.dropna(subset = \"SynthLogRet\"))\n",
    "\n",
    "        com_reg = oes.regression(mkt_crsp,\"Company\")\n",
    "        ivol_reg = oes.regression(mkt_ivol,\"Synth\")\n",
    "\n",
    "        if type(ivol_reg) == str or type(com_reg) == str:\n",
    "            print(\"Reg Error\")\n",
    "            raise Error\n",
    "\n",
    "        com_after_regression += len(com_reg.dropna(subset = \"Tstat\"))\n",
    "        syn_after_regression += len(ivol_reg.dropna(subset = \"Tstat\"))\n",
    "\n",
    "        syn_df_reg = ivol_reg.rename(columns={\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\",\"ExplainedReturn\":\"SynExp\"}).drop(columns= \"Mkt_Lnrtn\")\n",
    "        com_df_reg = com_reg.rename(columns={\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\",\"ExplainedReturn\":\"ComExp\"}).drop(columns = \"CFACPR\")\n",
    "        summary = syn_df_reg.join(com_df_reg,how=\"right\")\n",
    "        summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "        summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "        for i in summary.index:                                                   \n",
    "            if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "                summary.loc[i,\"Redflag\"] = 1\n",
    "            \n",
    "            elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "                == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "                summary.loc[i,\"Redflag\"] = 0\n",
    "            \n",
    "            elif summary.loc[i,\"Equal\"] == True and \\\n",
    "                summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "                summary.loc[i,\"Redflag\"] = 1\n",
    "            \n",
    "            else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "                summary.loc[i,\"Redflag\"] = 0\n",
    "\n",
    "        summary.replace(True,1,inplace=True)\n",
    "        summary.replace(False,0,inplace=True)\n",
    "\n",
    "        summary.loc[:,\"DIVAMT\"] = mkt_crsp.DIVAMT\n",
    "        summary.loc[:,\"PERMCO\"] = mkt_crsp.PERMCO\n",
    "        summary.loc[:,\"TICKER\"] = mkt_crsp.TICKER\n",
    "        summary = summary[['TICKER','PERMCO','PRC','SynthPrice','DIVAMT','SynthLogRet','CompanyLogRet','SynExp','ComExp','SynResRtn','ComResRtn','SynTstat', 'ComTstat','SynSig','ComSig','StdErr_Synth','StdErr_Company','Equal','Direction', 'Redflag']]\n",
    "        print(summary.TICKER.unique()[0],\": Done\",len(summary.dropna(subset= \"Equal\")))\n",
    "\n",
    "        summary_count += len(summary.dropna(subset= \"Equal\"))\n",
    "\n",
    "        summary_path = f'{source}\\\\summary_{filt}'\n",
    "        if not os.path.exists(summary_path):\n",
    "            os.makedirs(f\"{source}\\\\summary_{filt}\")\n",
    "\n",
    "        sum_path = f\"{source}\\\\summary_{filt}\\\\{tic}_summary_{filt}.csv\"\n",
    "        summary.dropna(subset = \"SynResRtn\").to_csv(sum_path)\n",
    "        return(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\OES\\\\CFR\\\\crsp_ind\\\\10020_crsp.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(f\"{source}\\\\crsp_ind\\\\*_crsp.csv\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10020 ['CSBR']\n",
      "['CSBR'] ivol file unavailable\n",
      "10039 ['HIFS']\n",
      "['HIFS'] ivol file unavailable\n",
      "10042 ['TORM']\n",
      "['TORM'] ivol file unavailable\n",
      "10053 ['DRL']\n",
      "['DRL'] ivol file unavailable\n",
      "10059 ['LWAY']\n",
      "['LWAY'] ivol file unavailable\n",
      "10065 ['CEC']\n",
      "Reg Error - ivol\n",
      "10070 ['URRE', 'WWR']\n",
      "['URRE', 'WWR'] ivol file unavailable\n",
      "10104 ['BTUI']\n",
      "['BTUI'] ivol file unavailable\n",
      "10107 ['TECH']\n",
      "TECH : Done 1259\n",
      "10110 ['TRS']\n",
      "Reg Error - ivol\n",
      "10127 ['CLRO']\n",
      "['CLRO'] ivol file unavailable\n",
      "10147 ['HNR']\n",
      "HNR : Done 1008\n",
      "1015 ['STZ']\n",
      "STZ : Done 1259\n",
      "10171 ['SPLS']\n",
      "SPLS : Done 1008\n",
      "10181 ['ISDR']\n",
      "['ISDR'] ivol file unavailable\n",
      "10210 ['CRUS']\n",
      "CRUS : Done 1259\n",
      "10218 ['FRME']\n",
      "Reg Error - ivol\n",
      "10224 ['SYMC']\n",
      "SYMC : Done 1259\n",
      "10241 ['CRH']\n",
      "CRH : Done 1259\n",
      "10256 ['CGNX']\n",
      "CGNX : Done 1259\n",
      "10288 ['NEOG']\n",
      "NEOG : Done 1259\n",
      "10299 ['HALL']\n",
      "Reg Error - ivol\n",
      "10302 ['VTR']\n",
      "VTR : Done 1259\n",
      "10303 ['EA']\n",
      "EA : Done 1259\n",
      "10323 ['DGII']\n",
      "DGII : Done 1259\n",
      "1033 ['COHR']\n",
      "COHR : Done 1259\n",
      "10358 ['ZIXI']\n",
      "Reg Error - ivol\n",
      "10359 ['LSCC']\n",
      "LSCC : Done 1259\n",
      "1035 ['COKE']\n",
      "['COKE'] ivol file unavailable\n",
      "10360 ['SURG']\n",
      "Reg Error - ivol\n",
      "10380 ['IMGN']\n",
      "IMGN : Done 1259\n",
      "10390 ['BLFS']\n",
      "['BLFS'] ivol file unavailable\n",
      "10408 ['PMTC', 'PTC']\n",
      "PMTC : Done 1259\n",
      "10416 ['MFRI', 'PPIH']\n",
      "['MFRI', 'PPIH'] ivol file unavailable\n",
      "['MFRI', 'PPIH'] ivol file unavailable\n",
      "10419 ['GIII']\n",
      "GIII : Done 1259\n",
      "10421 ['GSBC']\n",
      "Reg Error - ivol\n",
      "10467 ['ICON']\n",
      "ICON : Done 1259\n",
      "10486 ['CSCO']\n",
      "CSCO : Done 1259\n",
      "10504 ['HOLX']\n",
      "HOLX : Done 1259\n",
      "10512 ['MMSI']\n",
      "Reg Error - ivol\n",
      "10544 ['TTI']\n",
      "TTI : Done 1259\n",
      "10545 ['VICR']\n",
      "Reg Error - ivol\n",
      "10561 ['GVA']\n",
      "GVA : Done 1259\n",
      "10563 ['KMPR']\n",
      "Reg Error - ivol\n",
      "10564 ['PLCC', 'VBIV']\n",
      "['PLCC', 'VBIV'] ivol file unavailable\n",
      "10566 ['ORB']\n",
      "ORB : Done 504\n",
      "10567 ['BEAV']\n",
      "BEAV : Done 1008\n",
      "10593 ['AVNR']\n",
      "AVNR : Done 504\n",
      "10594 ['AN']\n",
      "AN : Done 1259\n",
      "10634 ['XLNX']\n",
      "XLNX : Done 1259\n",
      "10637 ['ESBF']\n",
      "['ESBF'] ivol file unavailable\n",
      "10651 ['GTIM']\n",
      "Reg Error - ivol\n",
      "10652 ['ACAT']\n",
      "ACAT : Done 1008\n",
      "10669 ['WTSLA', 'WTSL']\n",
      "Reg Error - ivol\n",
      "10676 ['GRF']\n",
      "['GRF'] ivol file unavailable\n",
      "10684 ['FHCO', 'VERU']\n",
      "Reg Error - ivol\n",
      "10690 ['MOC']\n",
      "['MOC'] ivol file unavailable\n",
      "10691 ['TRMB']\n",
      "TRMB : Done 1259\n",
      "1071 ['CDE']\n",
      "CDE : Done 1259\n",
      "10724 ['EXPO']\n",
      "Reg Error - ivol\n",
      "1072 ['CUZ']\n",
      "Reg Error - ivol\n",
      "10743 ['PRK']\n",
      "Reg Error - ivol\n",
      "10758 ['MTRX']\n",
      "MTRX : Done 1259\n",
      "10759 ['ZOOM']\n",
      "Reg Error - ivol\n",
      "10805 ['CATY']\n",
      "CATY : Done 1259\n",
      "10815 ['TBAC']\n",
      "['TBAC'] ivol file unavailable\n",
      "10817 ['AAON']\n",
      "AAON : Done 1259\n",
      "10827 ['QDEL']\n",
      "Reg Error - ivol\n",
      "10828 ['HMA']\n",
      "HMA : Done 252\n",
      "10831 ['JOEZ', 'DFBG']\n",
      "['JOEZ', 'DFBG'] ivol file unavailable\n",
      "['JOEZ', 'DFBG'] ivol file unavailable\n",
      "10833 ['API']\n",
      "['API'] ivol file unavailable\n",
      "10843 ['SONC']\n",
      "SONC : Done 1259\n",
      "10855 ['DORM']\n",
      "DORM : Done 1259\n",
      "10860 ['ATML']\n",
      "ATML : Done 756\n",
      "10867 ['PICO']\n",
      "PICO : Done 1259\n",
      "10876 ['REGN']\n",
      "REGN : Done 1259\n",
      "10877 ['IO']\n",
      "IO : Done 1259\n",
      "10883 ['VIFL']\n",
      "['VIFL'] ivol file unavailable\n",
      "10894 ['RTK']\n",
      "Reg Error - ivol\n",
      "10895 ['VPCO']\n",
      "Reg Error - ivol\n",
      "10901 ['PMCS']\n",
      "PMCS : Done 756\n",
      "10905 ['FCFS']\n",
      "Reg Error - ivol\n",
      "10924 ['AXAS']\n",
      "AXAS : Done 1259\n",
      "10937 ['ISIS', 'IONS']\n"
     ]
    }
   ],
   "source": [
    "syn_before_cleaning = 0\n",
    "com_before_cleaning = 0\n",
    "syn_after_cleaning = 0\n",
    "com_after_cleaning = 0\n",
    "syn_after_regression = 0\n",
    "com_after_regression = 0\n",
    "summary_count = 0\n",
    "com_reg_err = 0\n",
    "syn_reg_err = 0\n",
    "multi_tic_err = 0 \n",
    "ivol_unavail_err = 0\n",
    "\n",
    "volume_filter = False\n",
    "oi_filter = False\n",
    "zero_price_filter = True\n",
    "filter_9999 = True\n",
    "filt = \"no_filter\"\n",
    "fred = oes.fred(f\"{source}\\\\DGS1 1Yr Constant Mat Treasury.xlsx\")\n",
    "blr = oes.blr(f\"{source}\\\\BB BLR Index.xlsx\")\n",
    "marketdata = oes.marketdata(f\"{source}\\\\crsp_ind\\\\7_crsp.csv\")          #Taking Apple as a base for full dataset, since it is the most heavily traded stock\n",
    "\n",
    "for path_crsp in glob.glob(f\"{source}\\\\crsp_ind\\\\*_crsp.csv\"):\n",
    "    raw_df = oes.import_crsp(path_crsp)\n",
    "    tickers = raw_df.TICKER.unique().tolist()\n",
    "    permco = path_crsp.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "\n",
    "\n",
    "#Comment this section out if you want to include the PERMCOs with multiple tickers\n",
    "    # if len(tickers) > 1:\n",
    "    #     print(\"multi ticker\")\n",
    "    #     multi_tic_err += 1\n",
    "    #     continue\n",
    "    # tickers = tickers[0]\n",
    "\n",
    "\n",
    "    mkt_crsp = pd.DataFrame(index = marketdata.index.tolist())\n",
    "    mkt_ivol = pd.DataFrame(index = marketdata.index.tolist())\n",
    "    print(raw_df.PERMCO.unique()[0], tickers)\n",
    "\n",
    "    ivol_error = False\n",
    "\n",
    "    for tic in tickers:\n",
    "        tic_ivol = oes.import_ivol(tic,f\"{source}\\\\ivol_ind\\\\\")\n",
    "        \n",
    "        if len(tic_ivol) < 1:\n",
    "            print(f\"{tickers} ivol file unavailable\")\n",
    "            ivol_error = True\n",
    "            ivol_unavail_err += 1\n",
    "            continue\n",
    "        \n",
    "        tic_com_df = oes.return_check(oes.dupe_check(raw_df[raw_df.TICKER == tic].reset_index(),path_crsp),path_crsp).set_index(\"DataDate\")\n",
    "        \n",
    "        com_before_cleaning += len(tic_com_df.dropna(subset = \"RET\"))\n",
    "        cfacpr = tic_com_df.CFACPR\n",
    "        tic_ivol_df = oes.synthetic_stock(tic_ivol,cfacpr,fred,blr,volume_filter,oi_filter,zero_price_filter)\n",
    "        \n",
    "        mkt_crsp.loc[tic_com_df.index,\"PRC\"] = tic_com_df.PRC\n",
    "        mkt_crsp.loc[tic_com_df.index,\"TICKER\"] = tic_com_df.TICKER\n",
    "        mkt_crsp.loc[tic_com_df.index,\"RET\"] = tic_com_df.RET\n",
    "        mkt_crsp.loc[tic_com_df.index,\"CFACPR\"] = tic_com_df.CFACPR\n",
    "        mkt_crsp.loc[tic_com_df.index,\"PERMCO\"] = tic_com_df.PERMCO\n",
    "        mkt_crsp.loc[tic_com_df.index,\"DIVAMT\"] = tic_com_df.DIVAMT\n",
    "        mkt_crsp.loc[tic_com_df.index,\"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "        \n",
    "        mkt_ivol.loc[tic_com_df.index,\"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "        mkt_ivol.loc[tic_ivol_df.index,\"CFACPR\"] = tic_ivol_df.CFACPR\n",
    "        mkt_ivol.loc[tic_ivol_df.index,\"SynthPrice\"] = tic_ivol_df.SynthPrice\n",
    "    \n",
    "    if ivol_error == True:\n",
    "        continue\n",
    "\n",
    "    mkt_crsp.loc[:,\"CompanyLogRet\"] = ln(mkt_crsp.RET.astype(float) + 1)\n",
    "    mkt_ivol.loc[:,\"SynthLogRet\"] = ln(((mkt_ivol.SynthPrice*(mkt_ivol.CFACPR.shift(1)/mkt_ivol.CFACPR))/mkt_ivol.SynthPrice.shift(1)))\n",
    "    com_after_cleaning += len(mkt_crsp.dropna(subset = \"CompanyLogRet\"))\n",
    "    syn_after_cleaning += len(mkt_ivol.dropna(subset = \"SynthLogRet\"))\n",
    "\n",
    "    com_reg = oes.regression(mkt_crsp,\"Company\")\n",
    "    if type(com_reg) == str:\n",
    "        print(\"Reg Error - com\")\n",
    "        com_reg_err += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    ivol_reg = oes.regression(mkt_ivol,\"Synth\")\n",
    "\n",
    "    if type(ivol_reg) == str:\n",
    "        print(\"Reg Error - ivol\")\n",
    "        syn_reg_err +=1 \n",
    "        continue\n",
    "\n",
    "    com_after_regression += len(com_reg.dropna(subset = \"Tstat\"))\n",
    "    syn_after_regression += len(ivol_reg.dropna(subset = \"Tstat\"))\n",
    "\n",
    "    syn_df_reg = ivol_reg.rename(columns={\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\",\"ExplainedReturn\":\"SynExp\"}).drop(columns= \"Mkt_Lnrtn\")\n",
    "    com_df_reg = com_reg.rename(columns={\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\",\"ExplainedReturn\":\"ComExp\"}).drop(columns = \"CFACPR\")\n",
    "    summary = syn_df_reg.join(com_df_reg,how=\"right\")\n",
    "    summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "    summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "    for i in summary.index:                                                   \n",
    "        if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "            summary.loc[i,\"Redflag\"] = 1\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "            == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "            summary.loc[i,\"Redflag\"] = 0\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and \\\n",
    "            summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "            summary.loc[i,\"Redflag\"] = 1\n",
    "        \n",
    "        else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "            summary.loc[i,\"Redflag\"] = 0\n",
    "\n",
    "    summary.replace(True,1,inplace=True)\n",
    "    summary.replace(False,0,inplace=True)\n",
    "\n",
    "    summary.loc[:,\"DIVAMT\"] = mkt_crsp.DIVAMT\n",
    "    summary.loc[:,\"PERMCO\"] = mkt_crsp.PERMCO\n",
    "    summary.loc[:,\"TICKER\"] = mkt_crsp.TICKER\n",
    "    summary = summary[['TICKER','PERMCO','PRC','SynthPrice','DIVAMT','SynthLogRet','CompanyLogRet','SynExp','ComExp','SynResRtn','ComResRtn','SynTstat', 'ComTstat','SynSig','ComSig','StdErr_Synth','StdErr_Company','Equal','Direction', 'Redflag']]\n",
    "    print(summary.TICKER.unique()[0],\": Done\",len(summary.dropna(subset= \"Equal\")))\n",
    "\n",
    "    summary_count += len(summary.dropna(subset= \"Equal\"))\n",
    "\n",
    "    summary_path = f'{source}\\\\summary_{filt}'\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(f\"{source}\\\\summary_{filt}\")\n",
    "\n",
    "    sum_path = f\"{source}\\\\summary_{filt}\\\\{permco}_summary_{filt}.csv\"\n",
    "    summary.dropna(subset = \"SynResRtn\").to_csv(sum_path)\n",
    "\n",
    "counts = pd.DataFrame(index=['Common','Synth','Total'],data={\"BeforeCleaning\":[com_before_cleaning,syn_before_cleaning,nan],\"AfterCleaning\": [com_after_cleaning,syn_after_cleaning,nan],\"AfterRegression\":[com_after_regression,syn_after_regression,nan],\"Total\":[nan,nan,summary_count]})\n",
    "counts.to_csv(f\"{source}\\\\{filt}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(com_reg_err)\n",
    "print(syn_reg_err)\n",
    "print(multi_tic_err)\n",
    "print(ivol_unavail_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame()\n",
    "for path in glob.glob(\"C:\\\\OES\\\\summary_no_filter\\\\*_summary_no_filter.csv\"):\n",
    "    df = pd.read_csv(path).drop(columns=\"Unnamed: 0\")\n",
    "    data = pd.DataFrame({\"Ticker\":str(df.TICKER.unique()).replace(\"[\",\"\").replace(\"]\",\"\"),\\\n",
    "        \"PERMCO\":df.PERMCO.unique(),\\\n",
    "            \"Datapoints\":len(df),\\\n",
    "                \"Dividend\":df.DIVAMT.sum(),\\\n",
    "                    \"Equal%\":(df.Equal.sum()/len(df)),\\\n",
    "                        \"Direction%\":(df.Direction.sum()/len(df)),\\\n",
    "                            \"RedFlag%\":(df.Redflag.sum()/len(df))})\n",
    "    final = pd.concat([final,data]).reset_index(drop=True)\n",
    "\n",
    "final.to_csv(\"C:\\\\OES\\\\final_no_filter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"1-Year Treasury Rate\":fred.set_index(\"DataDate\").fred,\"Broker Dealer Rate\": blr.set_index(\"DataDate\").blr,\"Market Log Returns\":marketdata.Mkt_Lnrtn}).to_csv(\"Exhibit_2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4be3c2259e80903b778e3326c10bdf0288757834b4aaf3ad2a9fe63fcc19fe5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
