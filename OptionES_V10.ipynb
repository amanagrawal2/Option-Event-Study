{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Option Event Study**\n",
    "Code Written by: `Aman Agrawal` <br>\n",
    "Email: aagrawal2@babson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log as ln\n",
    "from numpy import nan\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import glob\n",
    "from numpy import sqrt\n",
    "pd.options.mode.chained_assignment = None\n",
    "source = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(path,low_memory_=True):\n",
    "    df = pd.read_csv(path,low_memory=low_memory_)\n",
    "    \n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns= 'Unnamed: 0')\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        df = df.rename(columns= {\"date\":\"DataDate\"})\n",
    "\n",
    "    df.DataDate = pd.to_datetime(df.DataDate)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_check(df,path_crsp,path_ivol):\n",
    "    dupes = df[df.DataDate.duplicated(keep=False)]\n",
    "    if len(dupes) == 0:\n",
    "        return df\n",
    "    \n",
    "    # if dupes.DIVAMT.isnull().values.any():\n",
    "    if len(dupes) > len(df)*0.03:\n",
    "        os.startfile(path_crsp)\n",
    "        os.startfile(path_ivol)\n",
    "        input(\"Press enter to continue...\")\n",
    "        corrected_df = import_csv(path_crsp)\n",
    "    \n",
    "    else:\n",
    "        corrected_df = df.groupby('DataDate').last()\n",
    "        divamt = df.groupby('DataDate').sum().DIVAMT\n",
    "        corrected_df.DIVAMT = divamt\n",
    "        corrected_df = corrected_df.reset_index()\n",
    "    \n",
    "    return(corrected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_check(df,path):\n",
    "    df = df.sort_values(by=\"DataDate\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    if df['RET'].iloc[0] == \"C\":\n",
    "        df.loc[0,'RET'] = nan\n",
    "    if \"C\" in df.RET.tolist():\n",
    "        os.startfile(path)\n",
    "        input(\"Press enter to continue...\")\n",
    "        df = import_csv(path)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crsp = import_csv(\"CRSP.csv\",low_memory_=False)\n",
    "# for i in list(crsp.dropna(subset=\"PERMCO\").PERMCO.unique()):\n",
    "#     df = crsp[crsp.PERMCO == i]\n",
    "#     df = df.sort_values(\"DataDate\").dropna(subset = \"PRC\").dropna(subset = \"TICKER\").reset_index().drop(columns= \"index\")\n",
    "#     tic = list(df.TICKER.unique())\n",
    "#     if len(tic) == 0:\n",
    "#         continue\n",
    "#     df.to_csv(f\"{source}\\\\crsp_\\\\{tic[0]}_crsp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketdata = pd.read_excel(\"MktData_MLM.xlsx\")                                  #Importing market log returns data\n",
    "marketdata = marketdata.rename(columns={\"date\":\"DataDate\"})                     #Renaming date column for consistency\n",
    "marketdata = marketdata[marketdata.DataDate.isin(pd.date_range(start = \"2012-12-30\",end=\"2017-12-31\"))]\n",
    "marketdata = marketdata.set_index('DataDate').rename(columns={\"vwretd_ln\":\"Mkt_Lnrtn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr = pd.read_excel(\"BB BLR Index.xlsx\", skiprows = 5)\\\n",
    "    .drop(columns = [\"Date\",\"PX_LAST.1\",\"PX_LAST\",\n",
    "        \"Unnamed: 2\"]).rename(columns={\"Date.1\":\n",
    "            \"DataDate\",\"rate\":\"blr\"})                                           #Importing bank loan rate data from an excel\n",
    "fred = pd.read_excel(\"DGS1 1Yr Constant Mat Treasury.xlsx\",skiprows=10)\\\n",
    "    .rename(columns={\"observation_date\":\"DataDate\",\"DGS1\":\"fred\"})              #Importing fred rate data from an excel\n",
    "fred.fred = fred.fred/100                                                       #Formatting the fred data for consistency\n",
    "fred = fred.fillna(method=\"ffill\")                                              #Filling NaN values with previous available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, dtype):\n",
    "    data_type = dtype + \"LogRet\"                                                #Adding a suffix to signify Log Returns \n",
    "    formula = data_type + \" ~  Mkt_Lnrtn\"                                       #Creating the formula for Regression\n",
    "    years = df.index.year.unique().tolist()                                     #Creating a list of years for yearly regression\n",
    "\n",
    "    if 2012 in years:\n",
    "        years.remove(2012)\n",
    "\n",
    "    df_reg = pd.DataFrame()                                                     #Empty df to concat yearly data to \n",
    "\n",
    "    for y in years:                                                             #Running a for loop for yearly regressions\n",
    "        df_year = df.loc[f\"{y}\"]                                                #Filtering for a specific year\n",
    "        try:\n",
    "            if len(df_year.dropna(subset=\"SynthLogRet\")) < 251:                                                  #Removing incomplete years \n",
    "                continue\n",
    "        except:\n",
    "            if len(df_year.dropna(subset=\"CompanyLogRet\")) < 251:                                                  #Removing incomplete years \n",
    "                continue\n",
    "        fitted = ols(formula, data = df_year).fit()                             #Running the regression on the filtered year  \n",
    "        explained_rtn = fitted.predict(exog = df_year)                               #Calculating the expected returns \n",
    "        df_year.loc[:,\"ExplainedReturn\"] = explained_rtn                              #Assigned a column to the expected returns \n",
    "        df_year.loc[:,\"ResRtn\"] = df_year[data_type] - explained_rtn                 #Calculating Residual Returns\n",
    "        df_year.loc[:,\"Tstat\"] = df_year.ResRtn/sqrt(fitted.scale)              #Calculating Tstat\n",
    "        df_year.loc[:,\"Sig\"] = abs(df_year.Tstat) > 1.96                        #Checking significance\n",
    "        df_year.loc[:,f\"StdErr_{dtype}\"] = fitted.bse[0]\n",
    "        df_reg = pd.concat([df_reg,df_year])                                    #Joining the yearly regressions  \n",
    "    if len(df_reg) == 0:\n",
    "        return(\"Lack of data\")\n",
    "    return(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_stock(syn_df,cfacpr):\n",
    "    \n",
    "    global syn_before_cleaning,syn_after_cleaning,syn_after_regression\n",
    "    \n",
    "    syn_before_cleaning += len(syn_df)\n",
    "    \n",
    "    if volume_filter == True:   \n",
    "        syn_df = syn_df[syn_df.Volume != 0]                                     #Filtering for volume if volume_filter is True\n",
    "    \n",
    "    if oi_filter == True:\n",
    "        syn_df = syn_df[syn_df.OpenInterest != 0]                               #Filtering for open interest if oi_filter is True\n",
    "\n",
    "    syn_df.DataDate = pd.to_datetime(syn_df.DataDate)                           #Converting dates from str to datetime\n",
    "    syn_df.Expiration = pd.to_datetime(syn_df.Expiration)                       #Converting dates from str to datetime\n",
    "\n",
    "    syn_df = syn_df.merge(fred, on = 'DataDate', how = \"left\")                  #Joining fred data with df\n",
    "    syn_df = syn_df.merge(blr, on = 'DataDate', how = \"left\")                   #Joining blr data with df\n",
    "    \n",
    "    syn_df.loc[:,\"Tau\"] = (syn_df.Expiration - syn_df.DataDate).dt.days         #Calculating Tau\n",
    "    syn_df.loc[:,\"YearFrac\"] =  syn_df.Tau/365                                  #Converting Tau into years\n",
    "\n",
    "    syn_df.loc[:,\"BuyDiscount\"] = ((1 + syn_df.fred) ** syn_df.YearFrac)        #Calculating Buy Discount\n",
    "    syn_df.loc[:,\"SellDiscount\"] = (1 + syn_df.blr) ** syn_df.YearFrac          #Calculating Sell Discount\n",
    "    \n",
    "    call = syn_df[syn_df.Type == \"call\"].drop(columns = \"Type\")                 #Splitting the df into calls and puts\n",
    "    call = call.rename(columns={'Last':'CallLast', 'Bid': \"CallBid\",\\\n",
    "        'Ask':\"CallAsk\", 'Volume':\"CallVolume\", 'OpenInterest':\\\n",
    "            'CallOpenInterest', 'IV':\"CallIV\", 'Delta':'CallDelta',\\\n",
    "                'Gamma':'CallGamma', 'Theta':'CallTheta', 'Vega':'CallVega'})   #Renaming the columns\n",
    "    \n",
    "    put = syn_df[syn_df.Type == \"put\"].drop(columns = \"Type\")                   #Splitting the df into calls and puts\n",
    "    put = put.rename(columns={'Last':'PutLast', 'Bid': \"PutBid\", \\\n",
    "        'Ask':\"PutAsk\", 'Volume':\"PutVolume\", 'OpenInterest':\\\n",
    "            'PutOpenInterest', 'IV':\"PutIV\", 'Delta':'PutDelta',\\\n",
    "                'Gamma':'PutGamma', 'Theta':'PutTheta', 'Vega':'PutVega'})      #Renaming the columns\n",
    "    \n",
    "    syn_df = call.merge(put,how = \"left\", on=[\"Expiration\",\"DataDate\",\\\n",
    "        \"Strike\", \"UnderlyingSymbol\",\"UnderlyingPrice\",\"BuyDiscount\",\\\n",
    "            \"SellDiscount\",\"YearFrac\",\"Tau\",\"blr\",\"fred\"])           #Merging the call and put df to make them parallel\n",
    "\n",
    "    if zero_price_filter == True:                                               #Filtering for zero price quotes if zero_price_filter is True \n",
    "        syn_df = syn_df[syn_df[\"CallBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"CallAsk\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutAsk\"] != 0]\n",
    "\n",
    "    syn_df = syn_df[syn_df[\"CallBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"CallAsk\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutAsk\"] != 9999]\n",
    "\n",
    "    syn_df.loc[:,\"Buy\"] = syn_df.Strike/syn_df.BuyDiscount                      #Calculating Buy price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"Sell\"] = syn_df.Strike/syn_df.SellDiscount                    #Calculating Sell price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"SynthAsk\"] = syn_df.CallAsk - \\\n",
    "        syn_df.PutBid + syn_df.Buy                                              #Calculating Synthetic stock's ask price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthBid\"] = syn_df.CallBid - \\\n",
    "        syn_df.PutAsk + syn_df.Sell                                             #Calculating Synthetic stock's sell price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthPrice\"] = (syn_df.SynthAsk + syn_df.SynthBid)/2          #Calculating Synthetic stock's price for the specific strike price on a day\n",
    "\n",
    "    syn_df = syn_df.groupby(\"DataDate\").mean()                                  #Calculating Synthetic stock's price on a day\n",
    "    \n",
    "    syn_df = syn_df.merge(cfacpr,on='DataDate',how='left')\n",
    "    \n",
    "    return(syn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etf_run(mkt_crsp,mkt_ivol,ivol_error,tic_com_df,path_ivol,path_crsp):\n",
    "    global com_before_cleaning, com_after_cleaning, syn_after_cleaning, com_after_regression , syn_after_regression, summary_count\n",
    "    com_before_cleaning += len(tic_com_df.dropna(subset = \"RET\"))\n",
    "    tic_com_df = return_check(dupe_check(tic_com_df,path_crsp,path_ivol),path_crsp).set_index(\"DataDate\")\n",
    "    cfacpr = tic_com_df.CFACPR\n",
    "    tic_ivol_df = synthetic_stock(import_csv(path_ivol),cfacpr)\n",
    "    \n",
    "    mkt_crsp.loc[tic_com_df.index,\"PRC\"] = tic_com_df.PRC\n",
    "    mkt_crsp.loc[tic_com_df.index,\"TICKER\"] = tic_com_df.TICKER\n",
    "    mkt_crsp.loc[tic_com_df.index,\"RET\"] = tic_com_df.RET\n",
    "    mkt_crsp.loc[tic_com_df.index,\"CFACPR\"] = tic_com_df.CFACPR\n",
    "    mkt_crsp.loc[tic_com_df.index,\"PERMCO\"] = tic_com_df.PERMCO\n",
    "    mkt_crsp.loc[tic_com_df.index,\"DIVAMT\"] = tic_com_df.DIVAMT\n",
    "    mkt_crsp.loc[marketdata.index, \"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "    \n",
    "    mkt_ivol.loc[marketdata.index,\"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "    mkt_ivol.loc[tic_ivol_df.index,\"CFACPR\"] = tic_ivol_df.CFACPR\n",
    "    mkt_ivol.loc[tic_ivol_df.index,\"SynthPrice\"] = tic_ivol_df.SynthPrice\n",
    "    if ivol_error == True:\n",
    "        return(None)\n",
    "    mkt_crsp.loc[:,\"CompanyLogRet\"] = ln(mkt_crsp.RET.astype(float) + 1)\n",
    "    mkt_ivol.loc[:,\"SynthLogRet\"] = ln(((mkt_ivol.SynthPrice*(mkt_ivol.CFACPR.shift(1)/mkt_ivol.CFACPR))/mkt_ivol.SynthPrice.shift(1)))\n",
    "    \n",
    "    com_after_cleaning += len(mkt_crsp.dropna(subset = \"CompanyLogRet\"))\n",
    "    syn_after_cleaning += len(mkt_ivol.dropna(subset = \"SynthLogRet\"))\n",
    "    \n",
    "    com_reg = regression(mkt_crsp,\"Company\")\n",
    "    ivol_reg = regression(mkt_ivol,\"Synth\")\n",
    "    \n",
    "    if type(ivol_reg) == str or type(com_reg) == str:\n",
    "        return(None)\n",
    "    \n",
    "    com_after_regression += len(com_reg.dropna(subset = \"Tstat\"))\n",
    "    syn_after_regression += len(ivol_reg.dropna(subset = \"Tstat\"))\n",
    "    \n",
    "    syn_df_reg = ivol_reg.rename(columns={\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\",\"ExplainedReturn\":\"SynExp\"}).drop(columns= \"Mkt_Lnrtn\")\n",
    "    com_df_reg = com_reg.rename(columns={\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\",\"ExplainedReturn\":\"ComExp\"}).drop(columns = \"CFACPR\")\n",
    "    summary = syn_df_reg.join(com_df_reg,how=\"right\")\n",
    "    summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "    summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "    for i in summary.index:                                                   \n",
    "        if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "            summary.loc[i,\"Redflag\"] = True\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "            == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "            summary.loc[i,\"Redflag\"] = False\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and \\\n",
    "            summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "            summary.loc[i,\"Redflag\"] = True\n",
    "        \n",
    "        else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "            summary.loc[i,\"Redflag\"] = False\n",
    "\n",
    "    summary.loc[:,\"DIVAMT\"] = mkt_crsp.DIVAMT\n",
    "    summary.loc[:,\"PERMCO\"] = mkt_crsp.PERMCO\n",
    "    summary.loc[:,\"TICKER\"] = mkt_crsp.TICKER\n",
    "    summary = summary[['TICKER','PERMCO','PRC','SynthPrice','DIVAMT','SynthLogRet','CompanyLogRet','SynExp','ComExp','SynResRtn','ComResRtn','SynTstat', 'ComTstat','SynSig','ComSig','StdErr_Synth','StdErr_Company','Equal','Direction', 'Redflag']]\n",
    "    print(summary.TICKER.unique()[0],\": Done\",summary.dropna(subset= \"Equal\").__len__())\n",
    "\n",
    "    summary_count += len(summary.dropna(subset= \"Equal\"))\n",
    "\n",
    "    summary_path = f'{source}\\\\summary_'\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(f\"{source}\\\\summary_\")\n",
    "    summary.dropna(subset = \"SynResRtn\").to_csv(f\"{source}\\\\summary_\\\\{summary.TICKER.unique()[0]}_summary_.csv\")\n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAAP']\n",
      "['AABA']\n",
      "['AACC']\n",
      "['AAC']\n",
      "AAC : Done 755\n",
      "['AAL']\n"
     ]
    }
   ],
   "source": [
    "syn_before_cleaning = 0\n",
    "com_before_cleaning = 0\n",
    "syn_after_cleaning = 0\n",
    "com_after_cleaning = 0\n",
    "syn_after_regression = 0\n",
    "com_after_regression = 0\n",
    "summary_count = 0\n",
    "volume_filter = False\n",
    "oi_filter = False\n",
    "zero_price_filter = True\n",
    "filter_9999 = True\n",
    "\n",
    "for path_crsp in glob.glob(f\"{source}\\\\crsp_\\\\*_crsp.csv\"):\n",
    "    raw_df = import_csv(path_crsp)\n",
    "    tickers = raw_df.TICKER.unique().tolist()\n",
    "    print(tickers)\n",
    "    mkt_crsp = pd.DataFrame(index = marketdata.index.tolist())\n",
    "    mkt_ivol = pd.DataFrame(index = marketdata.index.tolist())\n",
    "    \n",
    "    for tic in tickers:\n",
    "        path_ivol = f\"{source}\\\\ivol\\\\{tic}_ind.csv\"\n",
    "        ivol_error = False\n",
    "        if not os.path.exists(path_ivol):\n",
    "            ivol_error = True\n",
    "            continue\n",
    "        \n",
    "        tic_com_df = raw_df[raw_df.TICKER == tic].reset_index(drop = True)\n",
    "        \n",
    "        if str(tic_com_df.SHRCD.unique()[0])[0] == \"7\":\n",
    "            if len(tic_com_df.dropna(subset = \"RET\")) < 250:\n",
    "                continue\n",
    "            etf_run(mkt_crsp,mkt_ivol,ivol_error,tic_com_df,path_ivol,path_crsp)\n",
    "        \n",
    "        com_before_cleaning += len(tic_com_df.dropna(subset = \"RET\"))\n",
    "        \n",
    "        tic_com_df = return_check(dupe_check(tic_com_df,path_crsp,path_ivol),path_crsp).set_index(\"DataDate\")\n",
    "        cfacpr = tic_com_df.CFACPR\n",
    "        tic_ivol_df = synthetic_stock(import_csv(path_ivol),cfacpr)\n",
    "        \n",
    "        mkt_crsp.loc[tic_com_df.index,\"PRC\"] = tic_com_df.PRC\n",
    "        mkt_crsp.loc[tic_com_df.index,\"TICKER\"] = tic_com_df.TICKER\n",
    "        mkt_crsp.loc[tic_com_df.index,\"RET\"] = tic_com_df.RET\n",
    "        mkt_crsp.loc[tic_com_df.index,\"CFACPR\"] = tic_com_df.CFACPR\n",
    "        mkt_crsp.loc[tic_com_df.index,\"PERMCO\"] = tic_com_df.PERMCO\n",
    "        mkt_crsp.loc[tic_com_df.index,\"DIVAMT\"] = tic_com_df.DIVAMT\n",
    "        mkt_crsp.loc[marketdata.index, \"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "        \n",
    "        mkt_ivol.loc[marketdata.index,\"Mkt_Lnrtn\"] = marketdata.Mkt_Lnrtn\n",
    "        mkt_ivol.loc[tic_ivol_df.index,\"CFACPR\"] = tic_ivol_df.CFACPR\n",
    "        mkt_ivol.loc[tic_ivol_df.index,\"SynthPrice\"] = tic_ivol_df.SynthPrice\n",
    "    if ivol_error == True:\n",
    "        continue\n",
    "    mkt_crsp.loc[:,\"CompanyLogRet\"] = ln(mkt_crsp.RET.astype(float) + 1)\n",
    "    mkt_ivol.loc[:,\"SynthLogRet\"] = ln(((mkt_ivol.SynthPrice*(mkt_ivol.CFACPR.shift(1)/mkt_ivol.CFACPR))/mkt_ivol.SynthPrice.shift(1)))\n",
    "    \n",
    "    com_after_cleaning += len(mkt_crsp.dropna(subset = \"CompanyLogRet\"))\n",
    "    syn_after_cleaning += len(mkt_ivol.dropna(subset = \"SynthLogRet\"))\n",
    "    \n",
    "    com_reg = regression(mkt_crsp,\"Company\")\n",
    "    ivol_reg = regression(mkt_ivol,\"Synth\")\n",
    "    \n",
    "    if type(ivol_reg) == str or type(com_reg) == str:\n",
    "        continue\n",
    "    \n",
    "    com_after_regression += len(com_reg.dropna(subset = \"Tstat\"))\n",
    "    syn_after_regression += len(ivol_reg.dropna(subset = \"Tstat\"))\n",
    "    \n",
    "    syn_df_reg = ivol_reg.rename(columns={\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\",\"ExplainedReturn\":\"SynExp\"}).drop(columns= \"Mkt_Lnrtn\")\n",
    "    com_df_reg = com_reg.rename(columns={\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\",\"ExplainedReturn\":\"ComExp\"}).drop(columns = \"CFACPR\")\n",
    "    summary = syn_df_reg.join(com_df_reg,how=\"right\")\n",
    "    summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "    summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "    for i in summary.index:                                                   \n",
    "        if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "            summary.loc[i,\"Redflag\"] = 1\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "            == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "            summary.loc[i,\"Redflag\"] = 0\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and \\\n",
    "            summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "            summary.loc[i,\"Redflag\"] = 1\n",
    "        \n",
    "        else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "            summary.loc[i,\"Redflag\"] = 0\n",
    "\n",
    "    summary.replace(True,1,inplace=True)\n",
    "    summary.replace(False,0,inplace=True)\n",
    "\n",
    "    summary.loc[:,\"DIVAMT\"] = mkt_crsp.DIVAMT\n",
    "    summary.loc[:,\"PERMCO\"] = mkt_crsp.PERMCO\n",
    "    summary.loc[:,\"TICKER\"] = mkt_crsp.TICKER\n",
    "    summary = summary[['TICKER','PERMCO','PRC','SynthPrice','DIVAMT','SynthLogRet','CompanyLogRet','SynExp','ComExp','SynResRtn','ComResRtn','SynTstat', 'ComTstat','SynSig','ComSig','StdErr_Synth','StdErr_Company','Equal','Direction', 'Redflag']]\n",
    "    print(summary.TICKER.unique()[0],\": Done\",len(summary.dropna(subset= \"Equal\")))\n",
    "\n",
    "    summary_count += len(summary.dropna(subset= \"Equal\"))\n",
    "\n",
    "    summary_path = f'{source}\\\\summary_'\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(f\"{source}\\\\summary_\")\n",
    "    summary.dropna(subset = \"SynResRtn\").to_csv(f\"{source}\\\\summary_\\\\{summary.TICKER.unique()[0]}_summary_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6453629d7211d10f948ea67968a257c26208e64e999b29db1f6a7389fca678f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
