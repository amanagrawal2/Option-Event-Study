{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Option Event Study**\n",
    "Code Written by: `Aman Agrawal` <br>\n",
    "Email: aagrawal2@babson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log as ln\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import glob\n",
    "from numpy import sqrt\n",
    "from subprocess import Popen\n",
    "#pd.options.mode.chained_assignment = None\n",
    "source = os.path.abspath(os.getcwd())                                           #Getting folder directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketdata = pd.read_excel(\"MktData_MLM.xlsx\")                                  #Importing market log returns data\n",
    "marketdata = marketdata.rename(columns={\"date\":\"DataDate\"})                     #Renaming date column for consistency\n",
    "marketdata = marketdata[marketdata.DataDate.isin(pd.date_range(start = \"2012-12-30\",end=\"2017-12-31\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr = pd.read_excel(\"BB BLR Index.xlsx\", skiprows = 5)\\\n",
    "    .drop(columns = [\"Date\",\"PX_LAST.1\",\"PX_LAST\",\n",
    "        \"Unnamed: 2\"]).rename(columns={\"Date.1\":\n",
    "            \"DataDate\",\"rate\":\"blr\"})                                           #Importing bank loan rate data from an excel\n",
    "fred = pd.read_excel(\"DGS1 1Yr Constant Mat Treasury.xlsx\",skiprows=10)\\\n",
    "    .rename(columns={\"observation_date\":\"DataDate\",\"DGS1\":\"fred\"})              #Importing fred rate data from an excel\n",
    "fred.fred = fred.fred/100                                                       #Formatting the fred data for consistency\n",
    "fred = fred.fillna(method=\"ffill\")                                              #Filling NaN values with previous available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_before_cleaning = 0\n",
    "com_before_cleaning = 0\n",
    "syn_after_cleaning = 0\n",
    "com_after_cleaning = 0\n",
    "syn_before_regression = 0\n",
    "com_before_regression = 0\n",
    "syn_after_regression = 0\n",
    "com_after_regression = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Stock Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_filter = False\n",
    "oi_filter = False\n",
    "zero_price_filter = True\n",
    "filter_9999 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These filters are used to create the synthetic stock price. In order to use them, set their value as `True`.\n",
    "| Filter | Description|\n",
    "|---------|-----------|\n",
    "|`volume_filter`     | Removes zero volume contracts from the database |\n",
    "|`oi_filter`         | Removes zero open interest contracts from the database |\n",
    "|`zero_price_filter` | Removes contracts with zero price value. This happens when exchange provides no data for a specific contract | \n",
    "\n",
    "\n",
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, dtype):\n",
    "    data_type = dtype + \"LogRet\"                                                #Adding a suffix to signify Log Returns \n",
    "    formula = data_type + \" ~  Mkt_Lnrtn\"                                       #Creating the formula for Regression\n",
    "    years = df.index.year.unique().tolist()                                     #Creating a list of years for yearly regression\n",
    "\n",
    "    if 2012 in years:\n",
    "        years.remove(2012)\n",
    "\n",
    "    df_reg = pd.DataFrame()                                                     #Empty df to concat yearly data to \n",
    "\n",
    "    for y in years:                                                             #Running a for loop for yearly regressions\n",
    "        df_year = df.loc[f\"{y}\"]                                                #Filtering for a specific year\n",
    "        if len(df_year) < 252:                                                  #Removing incomplete years \n",
    "            continue\n",
    "        fitted = ols(formula, data = df_year).fit()                             #Running the regression on the filtered year  \n",
    "        expected = fitted.predict(exog = df_year)                               #Calculating the expected returns \n",
    "        df_year.loc[:,\"ExpectedReturn\"] = expected                              #Assigned a column to the expected returns \n",
    "        df_year.loc[:,\"ResRtn\"] = df_year[data_type] - expected                 #Calculating Residual Returns\n",
    "        df_year.loc[:,\"Tstat\"] = df_year.ResRtn/sqrt(fitted.scale)              #Calculating Tstat\n",
    "        df_year.loc[:,\"Sig\"] = abs(df_year.Tstat) > 1.96                        #Checking significance\n",
    "        df_reg = pd.concat([df_reg,df_year])                                    #Joining the yearly regressions  \n",
    "    return(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivol_paths = glob.glob(f\"{source}\\\\ind_1\\\\*_ind.csv\")\n",
    "c_error = []\n",
    "error = []\n",
    "for i in ivol_paths:\n",
    "    tic = i.split(\"\\\\\")[-1].split(\"_\")[0]\n",
    "    print(tic)\n",
    "\n",
    "    #Importing ivol and crsp data\n",
    "    syn_df = pd.read_csv(i).drop(columns=\"Unnamed: 0\")\n",
    "    \n",
    "    try:\n",
    "        com_df = pd.read_csv(f\"{source}\\\\crsp\\\\{tic}_ind.csv\").drop(columns=\"Unnamed: 0\")\n",
    "    except:\n",
    "        print(\"Not found \",tic)\n",
    "        error.append(f\"{tic} : No crsp data\")\n",
    "        continue\n",
    "\n",
    "    #Keeping count\n",
    "    com_before_cleaning += len(com_df)\n",
    "\n",
    "    #Cleaning common stock data\n",
    "    com_df.PRC = abs(com_df.PRC)\n",
    "    com_df=com_df.rename(columns = {'date':'DataDate'})\n",
    "    com_df.DataDate = pd.to_datetime(com_df.DataDate)\n",
    "    com_df.DISTCD = com_df.DISTCD.fillna(value=\"nan\")\n",
    "    #Duplicate date check\n",
    "    dupe = com_df[com_df.DataDate.duplicated(keep=False)]\n",
    "    \n",
    "    if len(dupe) != 0:\n",
    "        # print(f\"{tic} has duplicate dates.\")\n",
    "        # os.startfile(f\"{source}\\\\crsp\\\\{tic}_ind.csv\")\n",
    "        # input(\"Press enter to continue...\")\n",
    "        print(\"                        \", tic)\n",
    "\n",
    "    cfacpr = com_df[['DataDate','CFACPR']]\n",
    "\n",
    "    #Calculating Common Stock's Log returns\n",
    "    #com_df.loc[:,\"SynthLogRet\"] = ln((com_df.PRC*(com_df.CFACPR.shift(1)/com_df.CFACPR))/com_df.PRC.shift(1))\n",
    "    try:\n",
    "        print(tic)\n",
    "        if com_df.RET[0] == \"C\":\n",
    "            print(tic)\n",
    "            com_df.loc[0,\"RET\"] = 0\n",
    "        print(com_df.RET[:5])\n",
    "        com_df.loc[:,\"SynthLogRet\"] = ln(com_df.RET + 1)\n",
    "        continue\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        c_error.append(tic)\n",
    "        # os.startfile(f\"{source}\\\\crsp\\\\{tic}_ind.csv\")\n",
    "        # input(\"Press enter to continue...\")\n",
    "        # continue\n",
    "    \n",
    "    #Keeping count\n",
    "    syn_before_cleaning += len(syn_df)\n",
    "    \n",
    "    #filtering out the dataframe \n",
    "    if volume_filter == True:   \n",
    "        syn_df = syn_df[syn_df.Volume != 0]\n",
    "\n",
    "    if oi_filter == True:\n",
    "        syn_df = syn_df[syn_df.OpenInterest != 0]\n",
    "\n",
    "    if zero_price_filter == True:\n",
    "        syn_df = syn_df[syn_df[\"Bid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"Ask\"] != 0]\n",
    "\n",
    "    if filter_9999 == True:\n",
    "        syn_df = syn_df[syn_df[\"Bid\"] != 9999]\n",
    "        syn_df = syn_df[syn_df[\"Ask\"] != 9999]\n",
    "\n",
    "    #Converting dates from str to datetime\n",
    "    syn_df.DataDate = pd.to_datetime(syn_df.DataDate)\n",
    "    syn_df.Expiration = pd.to_datetime(syn_df.Expiration)\n",
    "\n",
    "    #Joining fred and blr data with df\n",
    "    syn_df = syn_df.merge(fred, on = 'DataDate', how = \"left\")\n",
    "    syn_df = syn_df.merge(blr, on = 'DataDate', how = \"left\")\n",
    "    \n",
    "    #keeping count\n",
    "    syn_after_cleaning += len(syn_df)\n",
    "    \n",
    "    #Calculating Buy and Sell Discount with the help of Tau and Fraction of years\n",
    "    syn_df.loc[:,\"Tau\"] = (syn_df.Expiration - syn_df.DataDate).dt.days\n",
    "    syn_df.loc[:,\"YearFrac\"] =  syn_df.Tau/365\n",
    "    syn_df.loc[:,\"BuyDiscount\"] = ((1 + syn_df.fred) ** syn_df.YearFrac)\n",
    "    syn_df.loc[:,\"SellDiscount\"] = (1 + syn_df.blr) ** syn_df.YearFrac\n",
    "\n",
    "    #Reorganizing the data into desirable form.  \n",
    "    call = syn_df[syn_df.Type == \"call\"].drop(columns = \"Type\")\n",
    "    call = call.rename(columns={'Last':'CallLast', 'Bid': \"CallBid\",\\\n",
    "        'Ask':\"CallAsk\", 'Volume':\"CallVolume\", 'OpenInterest':\\\n",
    "            'CallOpenInterest', 'IV':\"CallIV\", 'Delta':'CallDelta',\\\n",
    "                'Gamma':'CallGamma', 'Theta':'CallTheta', 'Vega':'CallVega'}) \n",
    "\n",
    "    put = syn_df[syn_df.Type == \"put\"].drop(columns = \"Type\")\n",
    "    put = put.rename(columns={'Last':'PutLast', 'Bid': \"PutBid\", \\\n",
    "        'Ask':\"PutAsk\", 'Volume':\"PutVolume\", 'OpenInterest':\\\n",
    "            'PutOpenInterest', 'IV':\"PutIV\", 'Delta':'PutDelta',\\\n",
    "                'Gamma':'PutGamma', 'Theta':'PutTheta', 'Vega':'PutVega'})\n",
    "\n",
    "    syn_df = call.merge(put,how = \"left\", on=[\"Expiration\",\"DataDate\",\\\n",
    "        \"Strike\", \"UnderlyingSymbol\",\"UnderlyingPrice\",\"BuyDiscount\",\\\n",
    "            \"SellDiscount\",\"YearFrac\",\"Tau\",\"blr\",\"fred\"])\n",
    "\n",
    "    #Calculating average synth ask price\n",
    "    syn_df.loc[:,\"Buy\"] = syn_df.Strike/syn_df.BuyDiscount\n",
    "    syn_df.loc[:,\"Sell\"] = syn_df.Strike/syn_df.SellDiscount\n",
    "\n",
    "    syn_df.loc[:,\"SynthAsk\"] = syn_df.CallAsk - syn_df.PutBid + syn_df.Buy\n",
    "    syn_df.loc[:,\"SynthBid\"] = syn_df.CallBid - syn_df.PutAsk + syn_df.Sell\n",
    "\n",
    "    syn_df.loc[:,\"SynthPrice\"] = (syn_df.SynthAsk + syn_df.SynthBid)/2\n",
    "    syn_df = syn_df.groupby(\"DataDate\").mean()\n",
    "\n",
    "    #Joining the Syn data with market returns on MktRtn's DataDate. And bringing in the adjustment factor for splits.\n",
    "    syn_df = marketdata.merge(syn_df,on='DataDate',how = \"left\").rename(columns= {'vwretd_ln':'Mkt_Lnrtn'})\n",
    "    syn_df = syn_df.merge(cfacpr, on = \"DataDate\", how=\"right\")\n",
    "\n",
    "    #Calculating Synthetic Stock's Log returns\n",
    "    syn_df.loc[:,\"SynthLogRet\"] = ln((syn_df.SynthPrice*(syn_df.CFACPR.shift(1)/syn_df.CFACPR))/syn_df.SynthPrice.shift(1))\n",
    "\n",
    "    #Cleaning the data by removing unwanted columns and dropping NA values in Synth Log Return column.\n",
    "    syn_df = syn_df[['DataDate',\"SynthLogRet\",'Mkt_Lnrtn',\"SynthPrice\",\"UnderlyingPrice\"]].set_index(\"DataDate\").dropna(subset=\"SynthLogRet\")\n",
    "\n",
    "    #Keeping count\n",
    "    syn_before_regression += len(syn_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tic in c_error:\n",
    "        os.startfile(f\"{source}\\\\crsp\\\\{tic}_ind.csv\")\n",
    "        input(\"Press enter to continue...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = com_df[['CompanyLogRet','DataDate']]                               #Getting rid of extra data\n",
    "com_df.DataDate = pd.to_datetime(com_df.DataDate) \n",
    "com_df = com_df.merge(marketdata,on='DataDate',how=\"left\")\\\n",
    "    .rename(columns= {'vwretd_ln':'Mkt_Lnrtn'})                             #Joining common stock returns with market returns\n",
    "com_df[\"DataDate\"] = pd.to_datetime(com_df[\"DataDate\"])                     #Converting dates from str to datetime\n",
    "com_df = com_df.set_index(\"DataDate\").dropna()                              #Making DataDate as an index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6453629d7211d10f948ea67968a257c26208e64e999b29db1f6a7389fca678f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
