{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Option Event Study**\n",
    "Code Written by: `Aman Agrawal` <br>\n",
    "Email: aagrawal2@babson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log as ln\n",
    "from numpy import nan\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import glob\n",
    "from numpy import sqrt\n",
    "from subprocess import Popen\n",
    "pd.options.mode.chained_assignment = None\n",
    "source = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(path,low_memory_=True):\n",
    "    df = pd.read_csv(path,low_memory=low_memory_)\n",
    "    \n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns= 'Unnamed: 0')\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        df = df.rename(columns= {\"date\":\"DataDate\"})\n",
    "\n",
    "    df.DataDate = pd.to_datetime(df.DataDate)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_check(df,path_crsp,path_ivol):\n",
    "    dupes = df[df.DataDate.duplicated(keep=False)]\n",
    "    if len(dupes) == 0:\n",
    "        return df\n",
    "    \n",
    "    # if dupes.DIVAMT.isnull().values.any():\n",
    "    if len(dupes) > len(df)*0.03:\n",
    "        os.startfile(path_crsp)\n",
    "        os.startfile(path_ivol)\n",
    "        input(\"Press enter to continue...\")\n",
    "        corrected_df = import_csv(path_crsp)\n",
    "    \n",
    "    else:\n",
    "        corrected_df = df.groupby('DataDate').last()\n",
    "        divamt = df.groupby('DataDate').sum().DIVAMT\n",
    "        corrected_df.DIVAMT = divamt\n",
    "        corrected_df = corrected_df.reset_index()\n",
    "    \n",
    "    return(corrected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_check(df,path):\n",
    "    df = df.sort_values(by=\"DataDate\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    if df['RET'].iloc[0] == \"C\":\n",
    "        df.loc[0,'RET'] = nan\n",
    "    if \"C\" in df.RET.tolist():\n",
    "        os.startfile(path)\n",
    "        input(\"Press enter to continue...\")\n",
    "        df = import_csv(path)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsp = import_csv(\"CRSP.csv\",low_memory_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketdata = pd.read_excel(\"MktData_MLM.xlsx\")                                  #Importing market log returns data\n",
    "marketdata = marketdata.rename(columns={\"date\":\"DataDate\"})                     #Renaming date column for consistency\n",
    "marketdata = marketdata[marketdata.DataDate.isin(pd.date_range(start = \"2012-12-30\",end=\"2017-12-31\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr = pd.read_excel(\"BB BLR Index.xlsx\", skiprows = 5)\\\n",
    "    .drop(columns = [\"Date\",\"PX_LAST.1\",\"PX_LAST\",\n",
    "        \"Unnamed: 2\"]).rename(columns={\"Date.1\":\n",
    "            \"DataDate\",\"rate\":\"blr\"})                                           #Importing bank loan rate data from an excel\n",
    "fred = pd.read_excel(\"DGS1 1Yr Constant Mat Treasury.xlsx\",skiprows=10)\\\n",
    "    .rename(columns={\"observation_date\":\"DataDate\",\"DGS1\":\"fred\"})              #Importing fred rate data from an excel\n",
    "fred.fred = fred.fred/100                                                       #Formatting the fred data for consistency\n",
    "fred = fred.fillna(method=\"ffill\")                                              #Filling NaN values with previous available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_before_cleaning = 0\n",
    "com_before_cleaning = 0\n",
    "syn_after_cleaning = 0\n",
    "com_after_cleaning = 0\n",
    "syn_before_regression = 0\n",
    "com_before_regression = 0\n",
    "syn_after_regression = 0\n",
    "com_after_regression = 0\n",
    "c_error = []\n",
    "error = []\n",
    "syn_data = []\n",
    "volume_filter = False\n",
    "oi_filter = False\n",
    "zero_price_filter = True\n",
    "filter_9999 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, dtype):\n",
    "    data_type = dtype + \"LogRet\"                                                #Adding a suffix to signify Log Returns \n",
    "    formula = data_type + \" ~  Mkt_Lnrtn\"                                       #Creating the formula for Regression\n",
    "    years = df.index.year.unique().tolist()                                     #Creating a list of years for yearly regression\n",
    "\n",
    "    if 2012 in years:\n",
    "        years.remove(2012)\n",
    "\n",
    "    df_reg = pd.DataFrame()                                                     #Empty df to concat yearly data to \n",
    "\n",
    "    for y in years:                                                             #Running a for loop for yearly regressions\n",
    "        df_year = df.loc[f\"{y}\"]                                                #Filtering for a specific year\n",
    "        if len(df_year) < 251:                                                  #Removing incomplete years \n",
    "            continue\n",
    "        fitted = ols(formula, data = df_year).fit()                             #Running the regression on the filtered year  \n",
    "        explained_rtn = fitted.predict(exog = df_year)                               #Calculating the expected returns \n",
    "        df_year.loc[:,\"ExpectedReturn\"] = explained_rtn                              #Assigned a column to the expected returns \n",
    "        df_year.loc[:,\"ResRtn\"] = df_year[data_type] - explained_rtn                 #Calculating Residual Returns\n",
    "        df_year.loc[:,\"Tstat\"] = df_year.ResRtn/sqrt(fitted.scale)              #Calculating Tstat\n",
    "        df_year.loc[:,\"Sig\"] = abs(df_year.Tstat) > 1.96                        #Checking significance\n",
    "        df_year.loc[:,\"StdErr\"] = fitted.bse\n",
    "        df_reg = pd.concat([df_reg,df_year])                                    #Joining the yearly regressions  \n",
    "    if len(df_reg) == 0:\n",
    "        return(\"Lack of data\")\n",
    "    return(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_stock(syn_df,cfacpr):\n",
    "    \n",
    "    if volume_filter == True:   \n",
    "        syn_df = syn_df[syn_df.Volume != 0]                                     #Filtering for volume if volume_filter is True\n",
    "    \n",
    "    if oi_filter == True:\n",
    "        syn_df = syn_df[syn_df.OpenInterest != 0]                               #Filtering for open interest if oi_filter is True\n",
    "\n",
    "    syn_df.DataDate = pd.to_datetime(syn_df.DataDate)                           #Converting dates from str to datetime\n",
    "    syn_df.Expiration = pd.to_datetime(syn_df.Expiration)                       #Converting dates from str to datetime\n",
    "\n",
    "    syn_df = syn_df.merge(fred, on = 'DataDate', how = \"left\")                  #Joining fred data with df\n",
    "    syn_df = syn_df.merge(blr, on = 'DataDate', how = \"left\")                   #Joining blr data with df\n",
    "    \n",
    "    syn_df.loc[:,\"Tau\"] = (syn_df.Expiration - syn_df.DataDate).dt.days         #Calculating Tau\n",
    "    syn_df.loc[:,\"YearFrac\"] =  syn_df.Tau/365                                  #Converting Tau into years\n",
    "\n",
    "    syn_df.loc[:,\"BuyDiscount\"] = ((1 + syn_df.fred) ** syn_df.YearFrac)        #Calculating Buy Discount\n",
    "    syn_df.loc[:,\"SellDiscount\"] = (1 + syn_df.blr) ** syn_df.YearFrac          #Calculating Sell Discount\n",
    "    \n",
    "    call = syn_df[syn_df.Type == \"call\"].drop(columns = \"Type\")                 #Splitting the df into calls and puts\n",
    "    call = call.rename(columns={'Last':'CallLast', 'Bid': \"CallBid\",\\\n",
    "        'Ask':\"CallAsk\", 'Volume':\"CallVolume\", 'OpenInterest':\\\n",
    "            'CallOpenInterest', 'IV':\"CallIV\", 'Delta':'CallDelta',\\\n",
    "                'Gamma':'CallGamma', 'Theta':'CallTheta', 'Vega':'CallVega'})   #Renaming the columns\n",
    "    \n",
    "    put = syn_df[syn_df.Type == \"put\"].drop(columns = \"Type\")                   #Splitting the df into calls and puts\n",
    "    put = put.rename(columns={'Last':'PutLast', 'Bid': \"PutBid\", \\\n",
    "        'Ask':\"PutAsk\", 'Volume':\"PutVolume\", 'OpenInterest':\\\n",
    "            'PutOpenInterest', 'IV':\"PutIV\", 'Delta':'PutDelta',\\\n",
    "                'Gamma':'PutGamma', 'Theta':'PutTheta', 'Vega':'PutVega'})      #Renaming the columns\n",
    "    \n",
    "    syn_df = call.merge(put,how = \"left\", on=[\"Expiration\",\"DataDate\",\\\n",
    "        \"Strike\", \"UnderlyingSymbol\",\"UnderlyingPrice\",\"BuyDiscount\",\\\n",
    "            \"SellDiscount\",\"YearFrac\",\"Tau\",\"blr\",\"fred\"])           #Merging the call and put df to make them parallel\n",
    "\n",
    "    if zero_price_filter == True:                                               #Filtering for zero price quotes if zero_price_filter is True \n",
    "        syn_df = syn_df[syn_df[\"CallBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"CallAsk\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutAsk\"] != 0]\n",
    "\n",
    "    syn_df = syn_df[syn_df[\"CallBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"CallAsk\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutAsk\"] != 9999]\n",
    "\n",
    "    syn_df.loc[:,\"Buy\"] = syn_df.Strike/syn_df.BuyDiscount                      #Calculating Buy price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"Sell\"] = syn_df.Strike/syn_df.SellDiscount                    #Calculating Sell price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"SynthAsk\"] = syn_df.CallAsk - \\\n",
    "        syn_df.PutBid + syn_df.Buy                                              #Calculating Synthetic stock's ask price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthBid\"] = syn_df.CallBid - \\\n",
    "        syn_df.PutAsk + syn_df.Sell                                             #Calculating Synthetic stock's sell price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthPrice\"] = (syn_df.SynthAsk + syn_df.SynthBid)/2          #Calculating Synthetic stock's price for the specific strike price on a day\n",
    "\n",
    "    syn_df = syn_df.groupby(\"DataDate\").mean()                                  #Calculating Synthetic stock's price on a day\n",
    "    \n",
    "    syn_df = marketdata.merge(syn_df,on='DataDate',how = \"left\")\\\n",
    "        .rename(columns= {'vwretd_ln':'Mkt_Lnrtn'})                             #Joining Synthetic stock returns with market returns\n",
    "    \n",
    "    syn_df = syn_df.merge(cfacpr,on='DataDate',how='left')\n",
    "    \n",
    "    syn_df.loc[:,\"SynthLogRet\"] = \\\n",
    "        ln(((syn_df.SynthPrice*(syn_df.CFACPR.shift(1)/syn_df.CFACPR))/syn_df.SynthPrice.shift(1)))                      #Calculating Synthetic Stock's Log returns\n",
    "\n",
    "    syn_df = syn_df[['DataDate',\"SynthLogRet\",'Mkt_Lnrtn',\"SynthPrice\"]]\\\n",
    "        .set_index(\"DataDate\").dropna(subset=\"SynthLogRet\")                     #Dropping the NA values based upon Synth Log returns \n",
    "\n",
    "    syn_df_reg = regression(syn_df,dtype = \"Synth\")                             #Running yearly regressions \n",
    "    \n",
    "    return(syn_df_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(crsp.dropna(subset=\"PERMCO\").PERMCO.unique()):\n",
    "    df = crsp[crsp.PERMCO == i]\n",
    "    df = df.sort_values(\"DataDate\").dropna(subset = \"PRC\").dropna(subset = \"TICKER\").reset_index().drop(columns= \"index\")\n",
    "    tic = list(df.TICKER.unique())\n",
    "    if len(tic) == 0:\n",
    "        continue\n",
    "    df.to_csv(f\"{source}\\\\crsp_\\\\{tic[0]}_crsp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAP\n",
      "AAC\n",
      "AAL\n",
      "AAN\n",
      "AAOI\n",
      "AAON\n",
      "AAPL\n",
      "AAP\n",
      "AAT\n",
      "AAU\n",
      "AAVL\n",
      "ADVM\n",
      "AAV\n",
      "AAWW\n",
      "AA\n",
      "ARNC\n",
      "ABAX\n",
      "ABBV\n",
      "ABB\n",
      "ABCB\n",
      "ABCO\n",
      "ABC\n",
      "ABDC\n",
      "ABFS\n",
      "ARCB\n",
      "ABG\n",
      "ABMD\n",
      "ABM\n",
      "ABR\n",
      "ABTL\n",
      "AUTO\n",
      "ABT\n",
      "ABV\n",
      "ABEV\n",
      "ABX\n",
      "ABY\n",
      "AY\n",
      "AB\n",
      "ACAD\n",
      "ACAS\n",
      "ACAT\n",
      "ACCL\n",
      "ACCO\n",
      "ACC\n",
      "ACET\n",
      "ACE\n",
      "CB\n",
      "ACFN\n",
      "ACGL\n",
      "ACHC\n",
      "ACHN\n",
      "ACH\n",
      "ACIA\n",
      "ACIW\n",
      "ACI\n",
      "ARCH\n",
      "ACLS\n",
      "ACMP\n",
      "WPZ\n",
      "ACM\n",
      "ACN\n",
      "ACOR\n",
      "ACO\n",
      "ACPW\n",
      "PIOI\n",
      "ACRE\n",
      "ACRS\n",
      "ACRX\n",
      "ACSF\n",
      "ACTG\n",
      "ACUR\n",
      "ACW\n",
      "ACXM\n",
      "ADAP\n",
      "ADBE\n",
      "ADC\n",
      "ADEP\n",
      "ADES\n",
      "ADHD\n",
      "ARCT\n",
      "ADI\n",
      "ADK\n",
      "RHE\n",
      "ADMP\n",
      "ADMS\n",
      "ADM\n",
      "ADNC\n",
      "ADNT\n",
      "ADPT\n",
      "ADP\n",
      "ADRO\n",
      "ADSK\n",
      "ADS\n",
      "ADTN\n",
      "ADT\n",
      "ADUS\n",
      "ADVS\n",
      "ADXS\n",
      "AEC\n",
      "AEE\n",
      "AEGN\n",
      "AEGR\n",
      "AEG\n",
      "AEIS\n",
      "AEL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mg:\\OES\\fresh\\OptionES_V8.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m com_reg \u001b[39m=\u001b[39m regression(com_df,\u001b[39m\"\u001b[39m\u001b[39mCompany\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m ivol \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(ivol_path)\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m ivol_ \u001b[39m=\u001b[39m synthetic_stock(ivol,cfacpr_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(ivol_) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32mg:\\OES\\fresh\\OptionES_V8.ipynb Cell 16\u001b[0m in \u001b[0;36msynthetic_stock\u001b[1;34m(syn_df, cfacpr)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m syn_df \u001b[39m=\u001b[39m syn_df[syn_df[\u001b[39m\"\u001b[39m\u001b[39mCallAsk\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m9999\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m syn_df \u001b[39m=\u001b[39m syn_df[syn_df[\u001b[39m\"\u001b[39m\u001b[39mPutAsk\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m9999\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m syn_df\u001b[39m.\u001b[39mloc[:,\u001b[39m\"\u001b[39m\u001b[39mBuy\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m syn_df\u001b[39m.\u001b[39mStrike\u001b[39m/\u001b[39msyn_df\u001b[39m.\u001b[39mBuyDiscount                      \u001b[39m#Calculating Buy price for the bond\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m syn_df\u001b[39m.\u001b[39mloc[:,\u001b[39m\"\u001b[39m\u001b[39mSell\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m syn_df\u001b[39m.\u001b[39mStrike\u001b[39m/\u001b[39msyn_df\u001b[39m.\u001b[39mSellDiscount                    \u001b[39m#Calculating Sell price for the bond\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m syn_df\u001b[39m.\u001b[39mloc[:,\u001b[39m\"\u001b[39m\u001b[39mSynthAsk\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m syn_df\u001b[39m.\u001b[39mCallAsk \u001b[39m-\u001b[39m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/OES/fresh/OptionES_V8.ipynb#X21sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     syn_df\u001b[39m.\u001b[39mPutBid \u001b[39m+\u001b[39m syn_df\u001b[39m.\u001b[39mBuy                                              \u001b[39m#Calculating Synthetic stock's ask price\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    715\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 716\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1625\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1622\u001b[0m \u001b[39m# add a new item with the dtype setup\u001b[39;00m\n\u001b[0;32m   1623\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(indexer[\u001b[39m0\u001b[39m]):\n\u001b[0;32m   1624\u001b[0m     \u001b[39m# We are setting an entire column\u001b[39;00m\n\u001b[1;32m-> 1625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj[key] \u001b[39m=\u001b[39m value\n\u001b[0;32m   1626\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m \u001b[39melif\u001b[39;00m is_array_like(value):\n\u001b[0;32m   1628\u001b[0m     \u001b[39m# GH#42099\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3655\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   3653\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3654\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3832\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3823\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   3825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   3831\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3832\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[0;32m   3834\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   3835\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m   3836\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   3837\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[0;32m   3838\u001b[0m     ):\n\u001b[0;32m   3839\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   3840\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4532\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   4530\u001b[0m \u001b[39m# We should never get here with DataFrame value\u001b[39;00m\n\u001b[0;32m   4531\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Series):\n\u001b[1;32m-> 4532\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m   4534\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m   4535\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Soulpure\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:10990\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[1;34m(value, index)\u001b[0m\n\u001b[0;32m  10986\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_reindex_for_setitem\u001b[39m(value: DataFrame \u001b[39m|\u001b[39m Series, index: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m  10987\u001b[0m     \u001b[39m# reindex if necessary\u001b[39;00m\n\u001b[0;32m  10989\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mequals(index) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m> 10990\u001b[0m         \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39;49m_values\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m  10992\u001b[0m     \u001b[39m# GH#4107\u001b[39;00m\n\u001b[0;32m  10993\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for path_crsp in glob.glob(f\"{source}\\\\crsp_\\\\*_crsp.csv\"):\n",
    "    raw_df = import_csv(path_crsp)\n",
    "    if len(raw_df) < 250:\n",
    "        continue\n",
    "\n",
    "    tickers = raw_df.TICKER.unique().tolist()\n",
    "    \n",
    "    for tic in tickers:\n",
    "        print(tic)\n",
    "        path_ivol = f\"{source}\\\\ivol\\\\{tic}_ind.csv\"\n",
    "        \n",
    "        if not os.path.exists(path_ivol):\n",
    "            continue\n",
    "        \n",
    "        df = raw_df[raw_df.TICKER == tic].reset_index(drop = True)\n",
    "\n",
    "        df = dupe_check(df,path_crsp,path_ivol)\n",
    "\n",
    "        df = return_check(df,path_crsp)\n",
    "\n",
    "        df = marketdata.merge(df,on = \"DataDate\", how = \"left\").rename(columns= {'vwretd_ln':'Mkt_Lnrtn'}).sort_index()\n",
    "\n",
    "        try:\n",
    "            df.loc[:,\"CompanyLogRet\"] = ln(df.RET.astype(float) + 1)\n",
    "        except:\n",
    "            os.startfile(path_crsp)\n",
    "            continue\n",
    "        \n",
    "        cfacpr_ = df[['CFACPR','DataDate']]\n",
    "        \n",
    "        com_df = df[['CompanyLogRet','Mkt_Lnrtn','DataDate']].dropna()\n",
    "        com_df.set_index(\"DataDate\",inplace=True)\n",
    "        com_reg = regression(com_df,\"Company\")\n",
    "        \n",
    "        ivol = import_csv(path_ivol)\n",
    "        ivol_ = synthetic_stock(ivol,cfacpr_)\n",
    "        \n",
    "        if type(ivol_) == str:\n",
    "            continue\n",
    "        #Starting with summarization\n",
    "        syn_df_reg = ivol_[[\"ResRtn\",\"Tstat\",\"Sig\"]]                                   #Filtering for required data\n",
    "        \n",
    "        com_df_reg = com_reg[[\"ResRtn\",\"Tstat\",\"Sig\"]]  #Filtering for required data\n",
    "        \n",
    "        syn_df_reg = syn_df_reg.rename(columns=\\\n",
    "            {\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\"})               #Renaming columns for joining the dfs\n",
    "        \n",
    "        com_df_reg = com_df_reg.rename(columns=\\\n",
    "            {\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\"})               #Renaming columns for joining the dfs\n",
    "        \n",
    "        summary = syn_df_reg.merge(com_df_reg,\\\n",
    "            on=\"DataDate\",how = \"right\")                                                          #Joining the common and synthetic regression df based on \"DataDate\"(index)\n",
    "        \n",
    "        summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "        summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "        \n",
    "        for i in summary.index:                                                     #Checking for Redflags\n",
    "            \n",
    "            if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "                summary.loc[i,\"Redflag\"] = True\n",
    "            \n",
    "            elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "                == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "                summary.loc[i,\"Redflag\"] = False\n",
    "            \n",
    "            elif summary.loc[i,\"Equal\"] == True and \\\n",
    "                summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "                summary.loc[i,\"Redflag\"] = True\n",
    "            \n",
    "            else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "                summary.loc[i,\"Redflag\"] = False\n",
    "        \n",
    "        print(tic,\": Done\",summary.dropna().__len__())\n",
    "        \n",
    "        summary_path = f'{source}/summary_'                                          #File path for export of summary\n",
    "        if not os.path.exists(summary_path):                                        #Creating the folder summary if it doesn't exist in the filepath\n",
    "            os.makedirs(f\"{source}/summary_\")\n",
    "        summary.dropna(subset = \"SynResRtn\").to_csv(f\"{source}/summary_/{tic}_summary_.csv\")                       #Exporting a csv file for the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6453629d7211d10f948ea67968a257c26208e64e999b29db1f6a7389fca678f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
