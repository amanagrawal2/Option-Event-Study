{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Option Event Study**\n",
    "Code Written by: `Aman Agrawal` <br>\n",
    "Email: aagrawal2@babson.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log as ln\n",
    "from numpy import nan\n",
    "from statsmodels.formula.api import ols\n",
    "import os\n",
    "import glob\n",
    "from numpy import sqrt\n",
    "pd.options.mode.chained_assignment = None\n",
    "source = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv(path,low_memory_=True):\n",
    "    df = pd.read_csv(path,low_memory=low_memory_)\n",
    "    \n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df = df.drop(columns= 'Unnamed: 0')\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        df = df.rename(columns= {\"date\":\"DataDate\"})\n",
    "\n",
    "    df.DataDate = pd.to_datetime(df.DataDate)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupe_check(df,path_crsp,path_ivol):\n",
    "    dupes = df[df.DataDate.duplicated(keep=False)]\n",
    "    if len(dupes) == 0:\n",
    "        return df\n",
    "    \n",
    "    # if dupes.DIVAMT.isnull().values.any():\n",
    "    if len(dupes) > len(df)*0.03:\n",
    "        os.startfile(path_crsp)\n",
    "        os.startfile(path_ivol)\n",
    "        input(\"Press enter to continue...\")\n",
    "        corrected_df = import_csv(path_crsp)\n",
    "    \n",
    "    else:\n",
    "        corrected_df = df.groupby('DataDate').last()\n",
    "        divamt = df.groupby('DataDate').sum().DIVAMT\n",
    "        corrected_df.DIVAMT = divamt\n",
    "        corrected_df = corrected_df.reset_index()\n",
    "    \n",
    "    return(corrected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_check(df,path):\n",
    "    df = df.sort_values(by=\"DataDate\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    if df['RET'].iloc[0] == \"C\":\n",
    "        df.loc[0,'RET'] = nan\n",
    "    if \"C\" in df.RET.tolist():\n",
    "        os.startfile(path)\n",
    "        input(\"Press enter to continue...\")\n",
    "        df = import_csv(path)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crsp = import_csv(\"CRSP.csv\",low_memory_=False)\n",
    "# for i in list(crsp.dropna(subset=\"PERMCO\").PERMCO.unique()):\n",
    "#     df = crsp[crsp.PERMCO == i]\n",
    "#     df = df.sort_values(\"DataDate\").dropna(subset = \"PRC\").dropna(subset = \"TICKER\").reset_index().drop(columns= \"index\")\n",
    "#     tic = list(df.TICKER.unique())\n",
    "#     if len(tic) == 0:\n",
    "#         continue\n",
    "#     df.to_csv(f\"{source}\\\\crsp_\\\\{tic[0]}_crsp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketdata = pd.read_excel(\"MktData_MLM.xlsx\")                                  #Importing market log returns data\n",
    "marketdata = marketdata.rename(columns={\"date\":\"DataDate\"})                     #Renaming date column for consistency\n",
    "marketdata = marketdata[marketdata.DataDate.isin(pd.date_range(start = \"2012-12-30\",end=\"2017-12-31\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blr = pd.read_excel(\"BB BLR Index.xlsx\", skiprows = 5)\\\n",
    "    .drop(columns = [\"Date\",\"PX_LAST.1\",\"PX_LAST\",\n",
    "        \"Unnamed: 2\"]).rename(columns={\"Date.1\":\n",
    "            \"DataDate\",\"rate\":\"blr\"})                                           #Importing bank loan rate data from an excel\n",
    "fred = pd.read_excel(\"DGS1 1Yr Constant Mat Treasury.xlsx\",skiprows=10)\\\n",
    "    .rename(columns={\"observation_date\":\"DataDate\",\"DGS1\":\"fred\"})              #Importing fred rate data from an excel\n",
    "fred.fred = fred.fred/100                                                       #Formatting the fred data for consistency\n",
    "fred = fred.fillna(method=\"ffill\")                                              #Filling NaN values with previous available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(df, dtype):\n",
    "    data_type = dtype + \"LogRet\"                                                #Adding a suffix to signify Log Returns \n",
    "    formula = data_type + \" ~  Mkt_Lnrtn\"                                       #Creating the formula for Regression\n",
    "    years = df.index.year.unique().tolist()                                     #Creating a list of years for yearly regression\n",
    "\n",
    "    if 2012 in years:\n",
    "        years.remove(2012)\n",
    "\n",
    "    df_reg = pd.DataFrame()                                                     #Empty df to concat yearly data to \n",
    "\n",
    "    for y in years:                                                             #Running a for loop for yearly regressions\n",
    "        df_year = df.loc[f\"{y}\"]                                                #Filtering for a specific year\n",
    "        try:\n",
    "            if len(df_year.dropna(subset=\"SynthLogRet\")) < 251:                                                  #Removing incomplete years \n",
    "                continue\n",
    "        except:\n",
    "            if len(df_year.dropna(subset=\"CompanyLogRet\")) < 251:                                                  #Removing incomplete years \n",
    "                continue\n",
    "        fitted = ols(formula, data = df_year).fit()                             #Running the regression on the filtered year  \n",
    "        explained_rtn = fitted.predict(exog = df_year)                               #Calculating the expected returns \n",
    "        df_year.loc[:,\"ExplainedReturn\"] = explained_rtn                              #Assigned a column to the expected returns \n",
    "        df_year.loc[:,\"ResRtn\"] = df_year[data_type] - explained_rtn                 #Calculating Residual Returns\n",
    "        df_year.loc[:,\"Tstat\"] = df_year.ResRtn/sqrt(fitted.scale)              #Calculating Tstat\n",
    "        df_year.loc[:,\"Sig\"] = abs(df_year.Tstat) > 1.96                        #Checking significance\n",
    "        df_year.loc[:,f\"StdErr_{dtype}\"] = fitted.bse[0]\n",
    "        df_reg = pd.concat([df_reg,df_year])                                    #Joining the yearly regressions  \n",
    "    if len(df_reg) == 0:\n",
    "        return(\"Lack of data\")\n",
    "    return(df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_stock(syn_df,cfacpr):\n",
    "    \n",
    "    global syn_before_cleaning,syn_after_cleaning,syn_before_regression,syn_after_regression\n",
    "    \n",
    "    syn_before_cleaning += len(syn_df)\n",
    "    \n",
    "    if volume_filter == True:   \n",
    "        syn_df = syn_df[syn_df.Volume != 0]                                     #Filtering for volume if volume_filter is True\n",
    "    \n",
    "    if oi_filter == True:\n",
    "        syn_df = syn_df[syn_df.OpenInterest != 0]                               #Filtering for open interest if oi_filter is True\n",
    "\n",
    "    syn_df.DataDate = pd.to_datetime(syn_df.DataDate)                           #Converting dates from str to datetime\n",
    "    syn_df.Expiration = pd.to_datetime(syn_df.Expiration)                       #Converting dates from str to datetime\n",
    "\n",
    "    syn_df = syn_df.merge(fred, on = 'DataDate', how = \"left\")                  #Joining fred data with df\n",
    "    syn_df = syn_df.merge(blr, on = 'DataDate', how = \"left\")                   #Joining blr data with df\n",
    "    \n",
    "    syn_df.loc[:,\"Tau\"] = (syn_df.Expiration - syn_df.DataDate).dt.days         #Calculating Tau\n",
    "    syn_df.loc[:,\"YearFrac\"] =  syn_df.Tau/365                                  #Converting Tau into years\n",
    "\n",
    "    syn_df.loc[:,\"BuyDiscount\"] = ((1 + syn_df.fred) ** syn_df.YearFrac)        #Calculating Buy Discount\n",
    "    syn_df.loc[:,\"SellDiscount\"] = (1 + syn_df.blr) ** syn_df.YearFrac          #Calculating Sell Discount\n",
    "    \n",
    "    call = syn_df[syn_df.Type == \"call\"].drop(columns = \"Type\")                 #Splitting the df into calls and puts\n",
    "    call = call.rename(columns={'Last':'CallLast', 'Bid': \"CallBid\",\\\n",
    "        'Ask':\"CallAsk\", 'Volume':\"CallVolume\", 'OpenInterest':\\\n",
    "            'CallOpenInterest', 'IV':\"CallIV\", 'Delta':'CallDelta',\\\n",
    "                'Gamma':'CallGamma', 'Theta':'CallTheta', 'Vega':'CallVega'})   #Renaming the columns\n",
    "    \n",
    "    put = syn_df[syn_df.Type == \"put\"].drop(columns = \"Type\")                   #Splitting the df into calls and puts\n",
    "    put = put.rename(columns={'Last':'PutLast', 'Bid': \"PutBid\", \\\n",
    "        'Ask':\"PutAsk\", 'Volume':\"PutVolume\", 'OpenInterest':\\\n",
    "            'PutOpenInterest', 'IV':\"PutIV\", 'Delta':'PutDelta',\\\n",
    "                'Gamma':'PutGamma', 'Theta':'PutTheta', 'Vega':'PutVega'})      #Renaming the columns\n",
    "    \n",
    "    syn_df = call.merge(put,how = \"left\", on=[\"Expiration\",\"DataDate\",\\\n",
    "        \"Strike\", \"UnderlyingSymbol\",\"UnderlyingPrice\",\"BuyDiscount\",\\\n",
    "            \"SellDiscount\",\"YearFrac\",\"Tau\",\"blr\",\"fred\"])           #Merging the call and put df to make them parallel\n",
    "\n",
    "    if zero_price_filter == True:                                               #Filtering for zero price quotes if zero_price_filter is True \n",
    "        syn_df = syn_df[syn_df[\"CallBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutBid\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"CallAsk\"] != 0]\n",
    "        syn_df = syn_df[syn_df[\"PutAsk\"] != 0]\n",
    "\n",
    "    syn_df = syn_df[syn_df[\"CallBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutBid\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"CallAsk\"] != 9999]\n",
    "    syn_df = syn_df[syn_df[\"PutAsk\"] != 9999]\n",
    "\n",
    "    syn_df.loc[:,\"Buy\"] = syn_df.Strike/syn_df.BuyDiscount                      #Calculating Buy price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"Sell\"] = syn_df.Strike/syn_df.SellDiscount                    #Calculating Sell price for the bond\n",
    "    \n",
    "    syn_df.loc[:,\"SynthAsk\"] = syn_df.CallAsk - \\\n",
    "        syn_df.PutBid + syn_df.Buy                                              #Calculating Synthetic stock's ask price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthBid\"] = syn_df.CallBid - \\\n",
    "        syn_df.PutAsk + syn_df.Sell                                             #Calculating Synthetic stock's sell price\n",
    "    \n",
    "    syn_df.loc[:,\"SynthPrice\"] = (syn_df.SynthAsk + syn_df.SynthBid)/2          #Calculating Synthetic stock's price for the specific strike price on a day\n",
    "\n",
    "    syn_df = syn_df.groupby(\"DataDate\").mean()                                  #Calculating Synthetic stock's price on a day\n",
    "    \n",
    "    # syn_df = marketdata.merge(syn_df,on='DataDate',how = \"left\")\\\n",
    "    #     .rename(columns= {'vwretd_ln':'Mkt_Lnrtn'})                             #Joining Synthetic stock returns with market returns\n",
    "    \n",
    "    syn_df = syn_df.merge(cfacpr,on='DataDate',how='left')\n",
    "    \n",
    "    syn_after_cleaning += len(syn_df.dropna(subset='SynthPrice'))\n",
    "    \n",
    "    syn_df.loc[:,\"SynthLogRet\"] = ln(((syn_df.SynthPrice*(syn_df.CFACPR.shift(1)/syn_df.CFACPR))/syn_df.SynthPrice.shift(1)))                      #Calculating Synthetic Stock's Log returns\n",
    "\n",
    "    syn_df = syn_df[['DataDate',\"SynthLogRet\",\"SynthPrice\"]].dropna(subset=\"SynthLogRet\")                     #Dropping the NA values based upon Synth Log returns \n",
    "\n",
    "    # syn_before_regression += len(syn_df.dropna(subset = \"SynthLogRet\"))\n",
    "    # syn_df_reg = regression(syn_df,dtype = \"Synth\")                             #Running yearly regressions\n",
    "    # try:\n",
    "    #     syn_after_regression += len(syn_df_reg.dropna(subset = \"SynthLogRet\"))\n",
    "    # except:\n",
    "    #     return(syn_df)\n",
    "    return(syn_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_stock(import_csv(f\"{source}\\\\ivol\\\\AAAP_ind.csv\"),cfacpr = cfacpr_).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = marketdata.merge(synthetic_stock(import_csv(f\"{source}\\\\ivol\\\\AAAP_ind.csv\"),cfacpr = cfacpr_).reset_index(),how=\"left\",on=\"DataDate\").rename(columns= {'vwretd_ln':'Mkt_Lnrtn'}).set_index(\"DataDate\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PERMNO', 'DataDate', 'SHRCD', 'TICKER', 'COMNAM', 'SHRCLS', 'PERMCO',\n",
       "       'CUSIP', 'DISTCD', 'DIVAMT', 'MMCNT', 'BIDLO', 'ASKHI', 'PRC', 'VOL',\n",
       "       'RET', 'BID', 'ASK', 'SHROUT', 'CFACPR', 'CFACSHR', 'NUMTRD', 'vwretd'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAAP']\n",
      "['AAC']\n",
      "['AAL']\n",
      "['AAN']\n",
      "['AAOI']\n",
      "['AAON']\n",
      "['AAPL']\n"
     ]
    }
   ],
   "source": [
    "syn_before_cleaning = 0\n",
    "com_before_cleaning = 0\n",
    "syn_after_cleaning = 0\n",
    "com_after_cleaning = 0\n",
    "syn_before_regression = 0\n",
    "com_before_regression = 0\n",
    "syn_after_regression = 0\n",
    "com_after_regression = 0\n",
    "volume_filter = False\n",
    "oi_filter = False\n",
    "zero_price_filter = True\n",
    "filter_9999 = True\n",
    "\n",
    "for path_crsp in glob.glob(f\"{source}\\\\crsp_\\\\*_crsp.csv\"):\n",
    "    raw_df = import_csv(path_crsp)\n",
    "    if len(raw_df) < 250:\n",
    "        continue\n",
    "\n",
    "    tickers = raw_df.TICKER.unique().tolist()\n",
    "    print(tickers)\n",
    "    \n",
    "    for tic in tickers:\n",
    "        path_ivol = f\"{source}\\\\ivol\\\\{tic}_ind.csv\"\n",
    "        \n",
    "        if not os.path.exists(path_ivol):\n",
    "            continue\n",
    "        \n",
    "        com_df = raw_df[raw_df.TICKER == tic].reset_index(drop = True)\n",
    "        com_before_cleaning += len(com_df.dropna(subset= \"RET\"))\n",
    "        com_df = dupe_check(com_df,path_crsp,path_ivol)\n",
    "        com_df = return_check(com_df,path_crsp)\n",
    "        com_after_cleaning += len(com_df.dropna(subset= \"RET\"))\n",
    "        cfacpr_ = com_df[['CFACPR','DataDate']]\n",
    "        ivol_df = synthetic_stock(import_csv(path_ivol),cfacpr = cfacpr_)\n",
    "        ivol_df_ = marketdata.merge(ivol_df,on = \"DataDate\", how = \"left\").rename(columns= {'vwretd_ln':'Mkt_Lnrtn'}).sort_index()\n",
    "        df_ = marketdata.merge(com_df,on = \"DataDate\", how = \"left\").rename(columns= {'vwretd_ln':'Mkt_Lnrtn'}).sort_index()\n",
    "\n",
    "    try:\n",
    "        df_.loc[:,\"CompanyLogRet\"] = ln(df_.RET.astype(float) + 1)\n",
    "    except:\n",
    "        print(\"as\")\n",
    "        os.startfile(path_crsp)\n",
    "        input(\"press\")\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    com_df = df_[['CompanyLogRet','Mkt_Lnrtn','DataDate',\"PRC\",\"DIVAMT\"]].dropna()\n",
    "    com_df.set_index(\"DataDate\",inplace=True)\n",
    "    com_before_regression += len(com_df.dropna(subset = \"CompanyLogRet\"))\n",
    "    com_reg = regression(com_df,\"Company\")\n",
    "    ivol_df_.set_index(\"DataDate\",inplace=True)\n",
    "    ivol_reg = regression(ivol_df_,\"Synth\")\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        com_after_regression += len(com_reg.dropna(subset = \"CompanyLogRet\"))\n",
    "    except:\n",
    "        com_after_regression += 0\n",
    "\n",
    "        ivol_reg = synthetic_stock(import_csv(path_ivol),cfacpr_)\n",
    "        \n",
    "        if type(ivol_reg) == str:\n",
    "            continue\n",
    "        \n",
    "    #Starting with summarization\n",
    "    if type(ivol_reg) == str or type(com_reg) == str:\n",
    "        continue\n",
    "    \n",
    "    syn_df_reg = ivol_reg.rename(columns={\"ResRtn\":\"SynResRtn\",\"Tstat\":\"SynTstat\",\"Sig\":\"SynSig\",\"ExplainedReturn\":\"SynExp\"})\n",
    "    com_df_reg = com_reg.rename(columns={\"ResRtn\":\"ComResRtn\",\"Tstat\":\"ComTstat\",\"Sig\":\"ComSig\",\"ExplainedReturn\":\"ComExp\"})\n",
    "    summary = syn_df_reg.merge(com_df_reg,on=\"DataDate\",how = \"right\")\n",
    "    summary.loc[:,\"Equal\"] = summary.SynSig == summary.ComSig                   #Checking if both, syn and common, Tstats are significant\n",
    "    summary.loc[:,\"Direction\"] = (summary.SynTstat/summary.ComTstat) >= 0       #Checking if the direction of both Tstats are similar\n",
    "    \n",
    "    for i in summary.index:                                                   \n",
    "        if summary.loc[i,\"Equal\"] == False:                                     #Redflag = True, if significance is not equal \n",
    "            summary.loc[i,\"Redflag\"] = True\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and (summary.loc[i,\"SynSig\"]\\\n",
    "            == False or summary.loc[i,\"ComSig\"] == False):                      #Redflag = False, if significance is equal but both are not significant\n",
    "            summary.loc[i,\"Redflag\"] = False\n",
    "        \n",
    "        elif summary.loc[i,\"Equal\"] == True and \\\n",
    "            summary.loc[i,\"Direction\"] == False:                                #Redflag = True, if significance is equal (both significant) but are in opposite directions\n",
    "            summary.loc[i,\"Redflag\"] = True\n",
    "        \n",
    "        else:                                                                   #Redflag = False, if significance is equal and the direction is same\n",
    "            summary.loc[i,\"Redflag\"] = False\n",
    "    \n",
    "    summary.loc[:,'UnderlyingPrice'] = com_df.PRC\n",
    "    summary.loc[:,\"DIVAMT\"] = com_df.DIVAMT\n",
    "    summary = summary[['UnderlyingPrice','SynthPrice','DIVAMT','SynthLogRet','CompanyLogRet','SynExp','ComExp','SynResRtn','ComResRtn','SynTstat', 'ComTstat','SynSig','ComSig','StdErr_Synth','StdErr_Company','Equal','Direction', 'Redflag']]\n",
    "    print(tic,\": Done\",summary.dropna(subset= \"Equal\").__len__())\n",
    "    \n",
    "    summary_path = f'{source}\\\\summary_'\n",
    "    if not os.path.exists(summary_path):\n",
    "        os.makedirs(f\"{source}\\\\summary_\")\n",
    "    summary.dropna(subset = \"SynResRtn\").to_csv(f\"{source}\\\\summary_\\\\{tic}_summary_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(syn_before_cleaning)\n",
    "print(com_before_cleaning)\n",
    "print(syn_after_cleaning)\n",
    "print(com_after_cleaning)\n",
    "print(syn_before_regression)\n",
    "print(com_before_regression)\n",
    "print(syn_after_regression)\n",
    "print(com_after_regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6453629d7211d10f948ea67968a257c26208e64e999b29db1f6a7389fca678f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
